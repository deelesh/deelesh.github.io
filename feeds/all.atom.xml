<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>My Weblog</title><link href="http://deelesh.github.io/" rel="alternate"></link><link href="http://deelesh.github.io/feeds/all.atom.xml" rel="self"></link><id>http://deelesh.github.io/</id><updated>2016-07-16T00:00:00-07:00</updated><entry><title>Hello PySpark World</title><link href="http://deelesh.github.io/hello-pyspark-world.html" rel="alternate"></link><published>2016-07-16T00:00:00-07:00</published><author><name>Deelesh Mandloi</name></author><id>tag:deelesh.github.io,2016-07-16:hello-pyspark-world.html</id><summary type="html">&lt;p&gt;When learning &lt;a href="http://spark.apache,org"&gt;Apache Spark&lt;/a&gt;, the most common first example seems to be a program to count the number of words in a file. Let’s see how we can write such a program using the &lt;a href="http://spark.apache.org/docs/latest/api/python/index.html"&gt;Python &lt;span class="caps"&gt;API&lt;/span&gt;&lt;/a&gt; for Spark (PySpark). This post assumes that you have already installed Spark. If you need a refresher on how to install Spark on Windows, checkout this &lt;a href="pyspark-windows.html"&gt;post&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id="word-count-program"&gt;Word Count Program&lt;/h2&gt;
&lt;p&gt;In this post we will learn how to write a program that counts the number of words in a file. To achieve this, the program needs to read the entire file, split each line on space and count the frequency of each unique word. Since I did not want to include a special file whose words our program can count, I am counting the words in the same file that contains the source code of our program. The entire program is listed below&lt;/p&gt;
&lt;div class="gist"&gt;
    &lt;script src='https://gist.github.com/57778e3f3f4d4ff4a8e49985ca4dce06.js'&gt;&lt;/script&gt;
    &lt;noscript&gt;
        &lt;pre&gt;&lt;code&gt;'''Print the words and their frequencies in this file'''

import operator
import pyspark

def main():
    '''Program entry point'''

    #Intialize a spark context
    with pyspark.SparkContext("local", "PySparkWordCount") as sc:
        #Get a RDD containing lines from this script file  
        lines = sc.textFile(__file__)
        #Split each line into words and assign a frequency of 1 to each word
        words = lines.flatMap(lambda line: line.split(" ")).map(lambda word: (word, 1))
        #count the frequency for words
        counts = words.reduceByKey(operator.add)
        #Sort the counts in descending order based on the word frequency
        sorted_counts =  counts.sortBy(lambda x: x[1], False)
        #Get an iterator over the counts to print a word and its frequency
        for word,count in sorted_counts.toLocalIterator():
            print(u"{} --&gt; {}".format(word, count))

if __name__ == "__main__":
    main()&lt;/code&gt;&lt;/pre&gt;
    &lt;/noscript&gt;
&lt;/div&gt;
&lt;h2 id="running-word-count-program"&gt;Running Word Count Program&lt;/h2&gt;
&lt;p&gt;To run the Word Count program,&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Open a terminal window such as a Windows Command Prompt.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Change into your SPARK_HOME directory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run the &lt;code&gt;spark-submit&lt;/code&gt; utility and pass the full path to your Word Count program file as an argument.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For example, on my Windows laptop I used the following commands to run the Word Count program.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    cd %SPARK_HOME%
    bin\spark-submit c:\code\pyspark-hello-world.py
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="building-blocks-of-a-pyspark-program"&gt;Building Blocks of a PySpark Program&lt;/h2&gt;
&lt;p&gt;In order to understand how the Word Count program works, we need to first understand the basic building blocks of any PySpark program. A PySpark program can be written using the following workflow &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Import the &lt;a href="http://spark.apache.org/docs/latest/api/python/index.html"&gt;pyspark&lt;/a&gt; Python module.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create the &lt;a href="http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext"&gt;SparkContext&lt;/a&gt; by specifying the &lt;span class="caps"&gt;URL&lt;/span&gt; of the cluster on which to run your application and your application name.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use one or more methods of the &lt;code&gt;SparkContext&lt;/code&gt; to create a &lt;a href="http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds"&gt;resilient distributed dataset&lt;/a&gt; (&lt;span class="caps"&gt;RDD&lt;/span&gt;) from your big data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Apply one or more transformations on your RDDs to process your big data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Apply one or more actions on your RDDs to produce the outputs.    &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="how-the-word-count-program-works"&gt;How the Word Count Program Works&lt;/h2&gt;
&lt;p&gt;Let’s see how we apply the PySpark workflow in our Word Count program. We first import the &lt;code&gt;pyspark&lt;/code&gt; module along with the &lt;a href="http://docs.python.org/2/library/operator.html"&gt;operator&lt;/a&gt; module from the Python standard library as we need to later use the &lt;code&gt;add&lt;/code&gt; function from the &lt;code&gt;operator&lt;/code&gt; module.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;operator&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pyspark&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once the &lt;code&gt;pyspark&lt;/code&gt; module is imported, we create a &lt;code&gt;SparkContext&lt;/code&gt; instance passing in the special keyword string, &lt;code&gt;local&lt;/code&gt;, and the name of our application, &lt;code&gt;PySparkWordCount&lt;/code&gt;. The &lt;code&gt;local&lt;/code&gt; keyword tells Spark to run this program locally in the same process that is used to run our program. Realistically you will specify the &lt;span class="caps"&gt;URL&lt;/span&gt; of the Spark cluster on which your application should run and not use the &lt;code&gt;local&lt;/code&gt; keyword. The &lt;code&gt;SparkContext&lt;/code&gt; is created using the &lt;a href="http://docs.python.org/2/reference/compound_stmts.html#with"&gt;with statement&lt;/a&gt; as the &lt;code&gt;SparkContext&lt;/code&gt; needs to be closed when our program terminates.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;pyspark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SparkContext&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"local"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"PySparkWordCount"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="c1"&gt;#Get a RDD containing lines from this script file&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Using the &lt;a href="http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext.textFile"&gt;textFile&lt;/a&gt; method on the &lt;code&gt;SparkContext&lt;/code&gt; instance, we get a &lt;span class="caps"&gt;RDD&lt;/span&gt; containing all the lines from the program file. The path to the program file is obtained using &lt;code&gt;__file__&lt;/code&gt; name.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;lines&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;textFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;__file__&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We then apply two transformations to the &lt;code&gt;lines&lt;/code&gt; &lt;span class="caps"&gt;RDD&lt;/span&gt;. First we split each line using a space to get a &lt;span class="caps"&gt;RDD&lt;/span&gt; of all words in every line using the &lt;a href="http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.flatMap"&gt;flatMap&lt;/a&gt; transformation. Then we create a new &lt;span class="caps"&gt;RDD&lt;/span&gt; containing a list of  two value tuples where each tuple associates the number 1 with each word like &lt;code&gt;[(import 1), (operator, 1)]&lt;/code&gt; using the &lt;a href="http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map"&gt;map&lt;/a&gt; transformation. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lines&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flatMap&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;" "&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note the use of &lt;a href="http://docs.python.org/2/tutorial/controlflow.html#lambda-expressions"&gt;lambda expression&lt;/a&gt; in the &lt;code&gt;flatMap&lt;/code&gt; and &lt;code&gt;map&lt;/code&gt; transformations. Lambda expressions are used in Python to create anonymous functions at runtime without binding the functions to names. The above line could also be written as&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;split_line&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;" "&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;assign_one&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lines&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flatMap&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;split_line&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;assign_one&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you are not used to lambda expressions, defining functions and then passing in function names to Spark transformations might make your code easier to read. But the Spark documentation seems to use lambda expressions in all of the Python examples. So it is better to get used to lambda expressions.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Lambda expressions can have only one statement which returns the value. In case you need to have multiple statements in your functions, you need to use the pattern of defining explicit functions and passing in their names.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We then apply the &lt;a href="http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey"&gt;reduceByKey&lt;/a&gt; transformation to the &lt;code&gt;words&lt;/code&gt; &lt;span class="caps"&gt;RDD&lt;/span&gt; passing in the &lt;code&gt;add&lt;/code&gt; function from the &lt;code&gt;operator&lt;/code&gt; standard library module. This creates a new &lt;span class="caps"&gt;RDD&lt;/span&gt; that is like a dictionary with keys as unique words in the file and values as the frequency of the words.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduceByKey&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;operator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We then sort the &lt;code&gt;counts&lt;/code&gt; &lt;span class="caps"&gt;RDD&lt;/span&gt; in the descending order based on the frequency of unique words such that words with highest frequency are listed first by applying the &lt;a href="http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sortBy"&gt;sortyBy&lt;/a&gt; transformation.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;sorted_counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  &lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sortBy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Finally we get an iterator over the &lt;code&gt;sorted_counts&lt;/code&gt; &lt;span class="caps"&gt;RDD&lt;/span&gt; by applying the &lt;a href="http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.toLocalIterator"&gt;toLocalIterator&lt;/a&gt; action to print each unique word in the file and its frequency. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;sorted_counts&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;toLocalIterator&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;u"{} --&amp;gt; {}"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;We are using the &lt;code&gt;toLocalIterator&lt;/code&gt; action instead of the &lt;a href="http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collect"&gt;collect&lt;/a&gt; action as &lt;code&gt;collect&lt;/code&gt; will return the entire list in memory which might cause an out of memory error if the input file is really big. By using the &lt;code&gt;toLocalIterator&lt;/code&gt; action, our program will only hold a single word in memory at any time.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="summary"&gt;Summary&lt;/h2&gt;
&lt;p&gt;You can write PySpark programs by creating a &lt;code&gt;SparkContext&lt;/code&gt;, loading your big data as an &lt;span class="caps"&gt;RDD&lt;/span&gt;, applying one or more transformations to the RDDs to perform your processing and applying one or more actions to the processed RDDs to get the results.&lt;/p&gt;</summary><category term="pyspark"></category><category term="spark"></category><category term="python"></category></entry><entry><title>Getting Started with PySpark on Windows</title><link href="http://deelesh.github.io/pyspark-windows.html" rel="alternate"></link><published>2016-07-09T22:45:00-07:00</published><author><name>Deelesh Mandloi</name></author><id>tag:deelesh.github.io,2016-07-09:pyspark-windows.html</id><summary type="html">&lt;p&gt;I decided to teach myself how to work with big data and came across &lt;a href="http://spark.apache.org"&gt;Apache Spark&lt;/a&gt;. While I had heard of &lt;a href="http://hadoop.apache.org"&gt;Apache Hadoop&lt;/a&gt;, to use Hadoop for working with big data, I had to write code in Java which I was not really looking forward to as I love to write code in Python. Spark supports a Python programming &lt;span class="caps"&gt;API&lt;/span&gt; called &lt;a href="http://spark.apache.org/docs/latest/api/python/pyspark.html"&gt;PySpark&lt;/a&gt; that is actively maintained and was enough to convince me to start learning PySpark for working with big data.&lt;/p&gt;
&lt;p&gt;In this post, I describe how I got started with PySpark on Windows. My laptop is running Windows 10. So the screenshots are specific to Windows 10. I am also assuming that you are comfortable working with the Command Prompt on Windows. You do not have to be an expert, but you need to know how to start a Command Prompt and run commands such as those that help you move around your computer’s file system. In case you need a refresher, a quick &lt;a href="http://www.cs.princeton.edu/courses/archive/spr05/cos126/cmd-prompt.html"&gt;introduction&lt;/a&gt; might be handy.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Often times, many open source projects do not have good Windows support. So I had to first figure out if Spark and PySpark would work well on Windows. The official Spark &lt;a href="http://spark.apache.org/docs/latest/#downloading"&gt;documentation&lt;/a&gt; does mention about supporting Windows.   &lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id="installing-prerequisites"&gt;Installing Prerequisites&lt;/h2&gt;
&lt;p&gt;PySpark requires Java version 7 or later and Python version 2.6 or later. Let’s first check if they are already installed or install them and make sure that PySpark can work with these two components.&lt;/p&gt;
&lt;h3 id="java"&gt;Java&lt;/h3&gt;
&lt;p&gt;Java is used by many other software. So it is quite possible that a required version (in our case version 7 or later) is already available on your computer. To check if Java is available and find it’s version, open a Command Prompt and type the following command.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    java -version
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If Java is installed and configured to work from a Command Prompt, running the above command should print the information about the Java version to the console. For example, I got the following output on my laptop.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;   java version "1.8.0_92"
   Java(TM) SE Runtime Environment (build 1.8.0_92-b14)
   Java HotSpot(TM) 64-Bit Server VM (build 25.92-b14, mixed mode)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Instead if you get a message like&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    'java' is not recognized as an internal or external command, operable program or batch file.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It means you need to install Java. To do so, &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Go to the Java &lt;a href="http://www.oracle.com/technetwork/java/javase/downloads/"&gt;download&lt;/a&gt; page. In case the download link has changed, search for &lt;code&gt;Java SE Runtime Environment&lt;/code&gt; on the internet and you should be able to find the download page.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Click the &lt;em&gt;Download&lt;/em&gt; button beneath &lt;em&gt;&lt;span class="caps"&gt;JRE&lt;/span&gt;&lt;/em&gt; &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Accept the license agreement and download the latest version of &lt;code&gt;Java SE Runtime Environment&lt;/code&gt; installer. I suggest getting the exe for Windows x64 (such as &lt;code&gt;jre-8u92-windows-x64.exe&lt;/code&gt;) unless you are using a 32 bit version of Windows in which case you need to get the &lt;em&gt;Windows x86 Offline&lt;/em&gt; version.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run the installer. &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;After the installation is complete, close the Command Prompt if it was already open, open it and check if you can successfully run &lt;code&gt;java -version&lt;/code&gt; command.&lt;/p&gt;
&lt;h3 id="python"&gt;Python&lt;/h3&gt;
&lt;p&gt;Python is used by many other software. So it is quite possible that a required version (in our case version 2.6 or later) is already available on your computer. To check if Python is available and find it’s version, open a Command Prompt and type the following command.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    python --version
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If Python is installed and configured to work from a Command Prompt, running the above command should print the information about the Python version to the console. For example, I got the following output on my laptop.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;   Python 2.7.10
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Instead if you get a message like&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    'python' is not recognized as an internal or external command, operable program or batch file.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It means you need to install Python. To do so,&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Go to the Python &lt;a href="https://www.python.org/downloads/windows/"&gt;download&lt;/a&gt; page.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Click the &lt;em&gt;Latest Python 2 Release&lt;/em&gt; link.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Download the &lt;code&gt;Windows x86-64 MSI installer&lt;/code&gt; file. If you are using a 32 bit version of Windows download the &lt;code&gt;Windows x86 MSI installer&lt;/code&gt; file.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When you run the installer, on the &lt;strong&gt;Customize Python&lt;/strong&gt; section, make sure that the option &lt;em&gt;Add python.exe to Path&lt;/em&gt; is selected. If this option is not selected, some of the PySpark utilities such as &lt;code&gt;pyspark&lt;/code&gt; and &lt;code&gt;spark-submit&lt;/code&gt; might not work.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Add python.exe to Path when installing Python on Windows" src="images/python-install.png"/&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;After the installation is complete, close the Command Prompt if it was already open, open it and check if you can successfully run &lt;code&gt;python --version&lt;/code&gt; command.&lt;/p&gt;
&lt;h2 id="installing-apache-spark"&gt;Installing Apache Spark&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Go to the Spark &lt;a href="http://spark.apache.org/downloads.html"&gt;download&lt;/a&gt; page.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For &lt;em&gt;Choose a Spark release&lt;/em&gt;, select the latest stable release of Spark.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For &lt;em&gt;Choose a package type&lt;/em&gt;, select a version that is pre-built for the latest version of Hadoop such as &lt;em&gt;Pre-built for Hadoop 2.6&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For &lt;em&gt;Choose a download type&lt;/em&gt;, select &lt;em&gt;Direct Download&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Click the link next to &lt;em&gt;Download Spark&lt;/em&gt; to download a zipped tarball file ending in .tgz extension such as &lt;code&gt;spark-1.6.2-bin-hadoop2.6.tgz&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In order to install Apache Spark, there is no need to run any installer. You can extract the files from the downloaded tarball in any folder of your choice using the &lt;a href="http://www.7-zip.org/"&gt;7Zip&lt;/a&gt; tool. &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Make sure that the folder path and the folder name containing Spark files do not contain any spaces.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In my case, I created a folder called &lt;code&gt;spark&lt;/code&gt; on my C drive and extracted the zipped tarball in a folder called &lt;code&gt;spark-1.6.2-bin-hadoop2.6&lt;/code&gt;. So all Spark files are in a folder called &lt;code&gt;C:\spark\spark-1.6.2-bin-hadoop2.6&lt;/code&gt;. From now on, I will refer to this folder as &lt;code&gt;SPARK_HOME&lt;/code&gt; in this post.&lt;/p&gt;
&lt;p&gt;To test if your installation was successful, open a Command Prompt, change to SPARK_HOME directory and type &lt;code&gt;bin\pyspark&lt;/code&gt;. This should start the PySpark shell which can be used to interactively work with Spark. I got the following messages in the console after running &lt;code&gt;bin\pyspark&lt;/code&gt; command.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;Python&lt;/span&gt; &lt;span class="nt"&gt;2&lt;/span&gt;&lt;span class="nc"&gt;.7.10&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;default&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;May&lt;/span&gt; &lt;span class="nt"&gt;23&lt;/span&gt; &lt;span class="nt"&gt;2015&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;09&lt;/span&gt;&lt;span class="nd"&gt;:44:00&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;MSC&lt;/span&gt; &lt;span class="nx"&gt;v.1500&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt; &lt;span class="nx"&gt;bit&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;AMD64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; &lt;span class="nt"&gt;on&lt;/span&gt; &lt;span class="nt"&gt;win32&lt;/span&gt;
&lt;span class="nt"&gt;Type&lt;/span&gt; &lt;span class="s2"&gt;"help"&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"copyright"&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"credits"&lt;/span&gt; &lt;span class="nt"&gt;or&lt;/span&gt; &lt;span class="s2"&gt;"license"&lt;/span&gt; &lt;span class="nt"&gt;for&lt;/span&gt; &lt;span class="nt"&gt;more&lt;/span&gt; &lt;span class="nt"&gt;information&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;span class="nt"&gt;Using&lt;/span&gt; &lt;span class="nt"&gt;Spark&lt;/span&gt;&lt;span class="s1"&gt;'s default log4j profile: org/apache/spark/log4j-defaults.properties&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 15:44:10 INFO SparkContext: Running Spark version 1.6.2&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 15:44:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 15:44:10 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path&lt;/span&gt;
&lt;span class="s1"&gt;java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:355)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:370)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.util.Shell.&amp;lt;clinit&amp;gt;(Shell.java:363)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.util.StringUtils.&amp;lt;clinit&amp;gt;(StringUtils.java:79)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:104)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.security.Groups.&amp;lt;init&amp;gt;(Groups.java:86)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.security.Groups.&amp;lt;init&amp;gt;(Groups.java:66)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:280)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:271)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:248)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:763)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:748)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:621)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2198)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2198)&lt;/span&gt;
&lt;span class="s1"&gt;        at scala.Option.getOrElse(Option.scala:120)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2198)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.spark.SparkContext.&amp;lt;init&amp;gt;(SparkContext.scala:322)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.spark.api.java.JavaSparkContext.&amp;lt;init&amp;gt;(JavaSparkContext.scala:59)&lt;/span&gt;
&lt;span class="s1"&gt;        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)&lt;/span&gt;
&lt;span class="s1"&gt;        at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)&lt;/span&gt;
&lt;span class="s1"&gt;        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)&lt;/span&gt;
&lt;span class="s1"&gt;        at java.lang.reflect.Constructor.newInstance(Unknown Source)&lt;/span&gt;
&lt;span class="s1"&gt;        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)&lt;/span&gt;
&lt;span class="s1"&gt;        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)&lt;/span&gt;
&lt;span class="s1"&gt;        at py4j.Gateway.invoke(Gateway.java:214)&lt;/span&gt;
&lt;span class="s1"&gt;        at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)&lt;/span&gt;
&lt;span class="s1"&gt;        at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)&lt;/span&gt;
&lt;span class="s1"&gt;        at py4j.GatewayConnection.run(GatewayConnection.java:209)&lt;/span&gt;
&lt;span class="s1"&gt;        at java.lang.Thread.run(Unknown Source)&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 15:44:10 INFO SecurityManager: Changing view acls to: deel4986&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 15:44:10 INFO SecurityManager: Changing modify acls to: deel4986&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 15:44:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(deel4986); users with modify permissions: Set(deel4986)&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 15:44:11 INFO Utils: Successfully started service '&lt;/span&gt;&lt;span class="nt"&gt;sparkDriver&lt;/span&gt;&lt;span class="err"&gt;'&lt;/span&gt; &lt;span class="nt"&gt;on&lt;/span&gt; &lt;span class="nt"&gt;port&lt;/span&gt; &lt;span class="nt"&gt;53607&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;span class="nt"&gt;16&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;07&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;09&lt;/span&gt; &lt;span class="nt"&gt;15&lt;/span&gt;&lt;span class="nd"&gt;:44:11&lt;/span&gt; &lt;span class="nt"&gt;INFO&lt;/span&gt; &lt;span class="nt"&gt;Slf4jLogger&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;Slf4jLogger&lt;/span&gt; &lt;span class="nt"&gt;started&lt;/span&gt;
&lt;span class="nt"&gt;16&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;07&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;09&lt;/span&gt; &lt;span class="nt"&gt;15&lt;/span&gt;&lt;span class="nd"&gt;:44:11&lt;/span&gt; &lt;span class="nt"&gt;INFO&lt;/span&gt; &lt;span class="nt"&gt;Remoting&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;Starting&lt;/span&gt; &lt;span class="nt"&gt;remoting&lt;/span&gt;
&lt;span class="nt"&gt;16&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;07&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;09&lt;/span&gt; &lt;span class="nt"&gt;15&lt;/span&gt;&lt;span class="nd"&gt;:44:11&lt;/span&gt; &lt;span class="nt"&gt;INFO&lt;/span&gt; &lt;span class="nt"&gt;Remoting&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;Remoting&lt;/span&gt; &lt;span class="nt"&gt;started&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="nt"&gt;listening&lt;/span&gt; &lt;span class="nt"&gt;on&lt;/span&gt; &lt;span class="nt"&gt;addresses&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;akka.tcp&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="c1"&gt;//sparkDriverActorSystem@localhost:53620]&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;Utils&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Successfully&lt;/span&gt; &lt;span class="nx"&gt;started&lt;/span&gt; &lt;span class="nx"&gt;service&lt;/span&gt; &lt;span class="s1"&gt;'sparkDriverActorSystem'&lt;/span&gt; &lt;span class="k"&gt;on&lt;/span&gt; &lt;span class="nx"&gt;port&lt;/span&gt; &lt;span class="mi"&gt;53620&lt;/span&gt;&lt;span class="nx"&gt;.&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;SparkEnv&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Registering&lt;/span&gt; &lt;span class="nx"&gt;MapOutputTracker&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;SparkEnv&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Registering&lt;/span&gt; &lt;span class="nx"&gt;BlockManagerMaster&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;DiskBlockManager&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Created&lt;/span&gt; &lt;span class="kd"&gt;local&lt;/span&gt; &lt;span class="nx"&gt;directory&lt;/span&gt; &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;C&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="nx"&gt;Users&lt;/span&gt;&lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="nx"&gt;deel4986&lt;/span&gt;&lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="nx"&gt;AppData&lt;/span&gt;&lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="kd"&gt;Local&lt;/span&gt;&lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="nx"&gt;Temp&lt;/span&gt;&lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="nx"&gt;blockmgr&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="nx"&gt;c931369&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;8987&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="nx"&gt;e52&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;9562&lt;/span&gt;&lt;span class="na"&gt;-f3e561aad111&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;MemoryStore&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;MemoryStore&lt;/span&gt; &lt;span class="nx"&gt;started&lt;/span&gt; &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nx"&gt;capacity&lt;/span&gt; &lt;span class="mf"&gt;511.1&lt;/span&gt; &lt;span class="nx"&gt;MB&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;SparkEnv&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Registering&lt;/span&gt; &lt;span class="nx"&gt;OutputCommitCoordinator&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;Utils&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Successfully&lt;/span&gt; &lt;span class="nx"&gt;started&lt;/span&gt; &lt;span class="nx"&gt;service&lt;/span&gt; &lt;span class="s1"&gt;'SparkUI'&lt;/span&gt; &lt;span class="k"&gt;on&lt;/span&gt; &lt;span class="nx"&gt;port&lt;/span&gt; &lt;span class="mi"&gt;4040&lt;/span&gt;&lt;span class="nx"&gt;.&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;SparkUI&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Started&lt;/span&gt; &lt;span class="nx"&gt;SparkUI&lt;/span&gt; &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;http&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="c1"&gt;//localhost:4040&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;Executor&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Starting&lt;/span&gt; &lt;span class="nx"&gt;executor&lt;/span&gt; &lt;span class="nx"&gt;ID&lt;/span&gt; &lt;span class="nx"&gt;driver&lt;/span&gt; &lt;span class="k"&gt;on&lt;/span&gt; &lt;span class="nx"&gt;host&lt;/span&gt; &lt;span class="nx"&gt;localhost&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;Utils&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Successfully&lt;/span&gt; &lt;span class="nx"&gt;started&lt;/span&gt; &lt;span class="nx"&gt;service&lt;/span&gt; &lt;span class="s1"&gt;'org.apache.spark.network.netty.NettyBlockTransferService'&lt;/span&gt; &lt;span class="k"&gt;on&lt;/span&gt; &lt;span class="nx"&gt;port&lt;/span&gt; &lt;span class="mi"&gt;53657&lt;/span&gt;&lt;span class="nx"&gt;.&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;NettyBlockTransferService&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Server&lt;/span&gt; &lt;span class="nx"&gt;created&lt;/span&gt; &lt;span class="k"&gt;on&lt;/span&gt; &lt;span class="mi"&gt;53657&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;BlockManagerMaster&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Trying&lt;/span&gt; &lt;span class="k"&gt;to&lt;/span&gt; &lt;span class="nb"&gt;register&lt;/span&gt; &lt;span class="nx"&gt;BlockManager&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;BlockManagerMasterEndpoint&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Registering&lt;/span&gt; &lt;span class="nx"&gt;block&lt;/span&gt; &lt;span class="nx"&gt;manager&lt;/span&gt; &lt;span class="nx"&gt;localhost&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;53657&lt;/span&gt; &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="mf"&gt;511.1&lt;/span&gt; &lt;span class="nx"&gt;MB&lt;/span&gt; &lt;span class="nx"&gt;RAM&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;BlockManagerId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;driver&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;localhost&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;53657&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;BlockManagerMaster&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Registered&lt;/span&gt; &lt;span class="nx"&gt;BlockManager&lt;/span&gt;
&lt;span class="nx"&gt;Welcome&lt;/span&gt; &lt;span class="k"&gt;to&lt;/span&gt;
      &lt;span class="nx"&gt;____&lt;/span&gt;              &lt;span class="nx"&gt;__&lt;/span&gt;
     &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="nx"&gt;__&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;__&lt;/span&gt;  &lt;span class="nx"&gt;___&lt;/span&gt; &lt;span class="nx"&gt;_____&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;__&lt;/span&gt;
    &lt;span class="nx"&gt;_&lt;/span&gt;&lt;span class="o"&gt;\&lt;/span&gt; &lt;span class="o"&gt;\/&lt;/span&gt; &lt;span class="nx"&gt;_&lt;/span&gt; &lt;span class="o"&gt;\/&lt;/span&gt; &lt;span class="nx"&gt;_&lt;/span&gt; &lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="nx"&gt;__&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;  &lt;span class="s1"&gt;'_/&lt;/span&gt;
&lt;span class="s1"&gt;   /__ / .__/\_,_/_/ /_/\_\   version 1.6.2&lt;/span&gt;
&lt;span class="s1"&gt;      /_/&lt;/span&gt;

&lt;span class="s1"&gt;Using Python version 2.7.10 (default, May 23 2015 09:44:00)&lt;/span&gt;
&lt;span class="s1"&gt;SparkContext available as sc, HiveContext available as sqlContext.&lt;/span&gt;
&lt;span class="s1"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The last message provides a hint on how to work with Spark in the PySpark shell using the &lt;code&gt;sc&lt;/code&gt; or &lt;code&gt;sqlContext&lt;/code&gt; names. For example, typing &lt;code&gt;sc.version&lt;/code&gt; in the shell should print the version of Spark. You can exit from the PySpark shell in the same way you exit from any Python shell by typing &lt;code&gt;exit()&lt;/code&gt;. &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The PySpark shell outputs a few messages on exit. So you need to hit enter to get back to the Command Prompt.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="configuring-the-spark-installation"&gt;Configuring the Spark Installation&lt;/h2&gt;
&lt;p&gt;Starting the PySpark shell produces a lot of messages of type &lt;span class="caps"&gt;INFO&lt;/span&gt;, &lt;span class="caps"&gt;ERROR&lt;/span&gt; and &lt;span class="caps"&gt;WARN&lt;/span&gt;. In this section we will see how to remove these messages. &lt;/p&gt;
&lt;p&gt;By default, the Spark installation on Windows does not include the &lt;code&gt;winutils.exe&lt;/code&gt; utility that is used by Spark. If you do not tell your Spark installation where to look for &lt;code&gt;winutils.exe&lt;/code&gt;, you will see error messages when running the PySpark shell such as&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This error message does not prevent the PySpark shell from starting. However if you try to run a standalone Python script using the &lt;code&gt;bin\spark-submit&lt;/code&gt; utility, you will get an error. For example, try running the &lt;code&gt;wordcount.py&lt;/code&gt; script from the &lt;code&gt;examples&lt;/code&gt; folder in the Command Prompt when you are in the SPARK_HOME directory.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    bin\spark-submit examples\src\main\python\wordcount.py README.md
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;which produces the following error that also points to missing &lt;code&gt;winutils.exe&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;Using&lt;/span&gt; &lt;span class="nt"&gt;Spark&lt;/span&gt;&lt;span class="s1"&gt;'s default log4j profile: org/apache/spark/log4j-defaults.properties&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 16:23:27 INFO SparkContext: Running Spark version 1.6.2&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 16:23:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 16:23:27 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path&lt;/span&gt;
&lt;span class="s1"&gt;java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:355)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:370)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.util.Shell.&amp;lt;clinit&amp;gt;(Shell.java:363)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.util.StringUtils.&amp;lt;clinit&amp;gt;(StringUtils.java:79)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:104)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.security.Groups.&amp;lt;init&amp;gt;(Groups.java:86)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.security.Groups.&amp;lt;init&amp;gt;(Groups.java:66)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:280)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:271)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:248)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:763)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:748)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:621)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2198)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2198)&lt;/span&gt;
&lt;span class="s1"&gt;        at scala.Option.getOrElse(Option.scala:120)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2198)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.spark.SparkContext.&amp;lt;init&amp;gt;(SparkContext.scala:322)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.spark.api.java.JavaSparkContext.&amp;lt;init&amp;gt;(JavaSparkContext.scala:59)&lt;/span&gt;
&lt;span class="s1"&gt;        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)&lt;/span&gt;
&lt;span class="s1"&gt;        at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)&lt;/span&gt;
&lt;span class="s1"&gt;        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)&lt;/span&gt;
&lt;span class="s1"&gt;        at java.lang.reflect.Constructor.newInstance(Unknown Source)&lt;/span&gt;
&lt;span class="s1"&gt;        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)&lt;/span&gt;
&lt;span class="s1"&gt;        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)&lt;/span&gt;
&lt;span class="s1"&gt;        at py4j.Gateway.invoke(Gateway.java:214)&lt;/span&gt;
&lt;span class="s1"&gt;        at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)&lt;/span&gt;
&lt;span class="s1"&gt;        at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)&lt;/span&gt;
&lt;span class="s1"&gt;        at py4j.GatewayConnection.run(GatewayConnection.java:209)&lt;/span&gt;
&lt;span class="s1"&gt;        at java.lang.Thread.run(Unknown Source)&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 16:23:27 INFO SecurityManager: Changing view acls to: deel4986&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 16:23:27 INFO SecurityManager: Changing modify acls to: deel4986&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 16:23:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(deel4986); users with modify permissions: Set(deel4986)&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 16:23:28 INFO Utils: Successfully started service '&lt;/span&gt;&lt;span class="nt"&gt;sparkDriver&lt;/span&gt;&lt;span class="err"&gt;'&lt;/span&gt; &lt;span class="nt"&gt;on&lt;/span&gt; &lt;span class="nt"&gt;port&lt;/span&gt; &lt;span class="nt"&gt;59506&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;span class="nt"&gt;16&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;07&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;09&lt;/span&gt; &lt;span class="nt"&gt;16&lt;/span&gt;&lt;span class="nd"&gt;:23:28&lt;/span&gt; &lt;span class="nt"&gt;INFO&lt;/span&gt; &lt;span class="nt"&gt;Slf4jLogger&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;Slf4jLogger&lt;/span&gt; &lt;span class="nt"&gt;started&lt;/span&gt;
&lt;span class="nt"&gt;16&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;07&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;09&lt;/span&gt; &lt;span class="nt"&gt;16&lt;/span&gt;&lt;span class="nd"&gt;:23:28&lt;/span&gt; &lt;span class="nt"&gt;INFO&lt;/span&gt; &lt;span class="nt"&gt;Remoting&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;Starting&lt;/span&gt; &lt;span class="nt"&gt;remoting&lt;/span&gt;
&lt;span class="nt"&gt;16&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;07&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;09&lt;/span&gt; &lt;span class="nt"&gt;16&lt;/span&gt;&lt;span class="nd"&gt;:23:28&lt;/span&gt; &lt;span class="nt"&gt;INFO&lt;/span&gt; &lt;span class="nt"&gt;Remoting&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;Remoting&lt;/span&gt; &lt;span class="nt"&gt;started&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="nt"&gt;listening&lt;/span&gt; &lt;span class="nt"&gt;on&lt;/span&gt; &lt;span class="nt"&gt;addresses&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;akka.tcp&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="c1"&gt;//sparkDriverActorSystem@localhost:59519]&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;Utils&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Successfully&lt;/span&gt; &lt;span class="nx"&gt;started&lt;/span&gt; &lt;span class="nx"&gt;service&lt;/span&gt; &lt;span class="s1"&gt;'sparkDriverActorSystem'&lt;/span&gt; &lt;span class="k"&gt;on&lt;/span&gt; &lt;span class="nx"&gt;port&lt;/span&gt; &lt;span class="mi"&gt;59519&lt;/span&gt;&lt;span class="nx"&gt;.&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;SparkEnv&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Registering&lt;/span&gt; &lt;span class="nx"&gt;MapOutputTracker&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;SparkEnv&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Registering&lt;/span&gt; &lt;span class="nx"&gt;BlockManagerMaster&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;MemoryStore&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;MemoryStore&lt;/span&gt; &lt;span class="nx"&gt;started&lt;/span&gt; &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nx"&gt;capacity&lt;/span&gt; &lt;span class="mf"&gt;511.1&lt;/span&gt; &lt;span class="nx"&gt;MB&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;SparkEnv&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Registering&lt;/span&gt; &lt;span class="nx"&gt;OutputCommitCoordinator&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;Utils&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Successfully&lt;/span&gt; &lt;span class="nx"&gt;started&lt;/span&gt; &lt;span class="nx"&gt;service&lt;/span&gt; &lt;span class="s1"&gt;'SparkUI'&lt;/span&gt; &lt;span class="k"&gt;on&lt;/span&gt; &lt;span class="nx"&gt;port&lt;/span&gt; &lt;span class="mi"&gt;4040&lt;/span&gt;&lt;span class="nx"&gt;.&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;SparkUI&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Started&lt;/span&gt; &lt;span class="nx"&gt;SparkUI&lt;/span&gt; &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;http&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="c1"&gt;//localhost:4040&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt; &lt;span class="nx"&gt;ERROR&lt;/span&gt; &lt;span class="nx"&gt;SparkContext&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Error&lt;/span&gt; &lt;span class="nx"&gt;initializing&lt;/span&gt; &lt;span class="nx"&gt;SparkContext.&lt;/span&gt;
&lt;span class="nx"&gt;java.lang.NullPointerException&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;java.lang.ProcessBuilder.start&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Unknown&lt;/span&gt; &lt;span class="nx"&gt;Source&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.hadoop.util.Shell.runCommand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Shell.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;482&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.hadoop.util.Shell.run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Shell.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;455&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.hadoop.util.Shell&lt;/span&gt;&lt;span class="nv"&gt;$ShellCommandExecutor.execute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Shell.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;715&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.hadoop.fs.FileUtil.chmod&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;FileUtil.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;873&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.hadoop.fs.FileUtil.chmod&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;FileUtil.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;853&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.spark.util.Utils&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="bp"&gt;.&lt;/span&gt;&lt;span class="nx nx-Member"&gt;fetchFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Utils.scala&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;407&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.spark.SparkContext.addFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;SparkContext.scala&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1386&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.spark.SparkContext.addFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;SparkContext.scala&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1340&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.spark.SparkContext&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;$anonfun&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="bp"&gt;.&lt;/span&gt;&lt;span class="nx nx-Member"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;SparkContext.scala&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;491&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.spark.SparkContext&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;$anonfun&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="bp"&gt;.&lt;/span&gt;&lt;span class="nx nx-Member"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;SparkContext.scala&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;491&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;scala.collection.immutable.List.foreach&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;List&lt;/span&gt;&lt;span class="bp"&gt;.&lt;/span&gt;&lt;span class="nx nx-Member"&gt;scala&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;318&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.spark.SparkContext.&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;init&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;SparkContext.scala&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;491&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.spark.api.java.JavaSparkContext.&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;init&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;JavaSparkContext.scala&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;59&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;sun.reflect.NativeConstructorAccessorImpl.newInstance0&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Native&lt;/span&gt; &lt;span class="nx"&gt;Method&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;sun.reflect.NativeConstructorAccessorImpl.newInstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Unknown&lt;/span&gt; &lt;span class="nx"&gt;Source&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;sun.reflect.DelegatingConstructorAccessorImpl.newInstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Unknown&lt;/span&gt; &lt;span class="nx"&gt;Source&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;java.lang.reflect.Constructor.newInstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Unknown&lt;/span&gt; &lt;span class="nx"&gt;Source&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;py4j.reflection.MethodInvoker.invoke&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;MethodInvoker.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;234&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;py4j.reflection.ReflectionEngine.invoke&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ReflectionEngine.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;381&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;py4j.Gateway.invoke&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Gateway.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;214&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;py4j.commands.ConstructorCommand.invokeConstructor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ConstructorCommand.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;79&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;py4j.commands.ConstructorCommand.execute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ConstructorCommand.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;68&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;py4j.GatewayConnection.run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;GatewayConnection.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;209&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;java.lang.Thread.run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Unknown&lt;/span&gt; &lt;span class="nx"&gt;Source&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;29&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;SparkUI&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Stopped&lt;/span&gt; &lt;span class="nx"&gt;Spark&lt;/span&gt; &lt;span class="nx"&gt;web&lt;/span&gt; &lt;span class="nx"&gt;UI&lt;/span&gt; &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;http&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="c1"&gt;//localhost:4040&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;29&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;MapOutputTrackerMasterEndpoint&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;MapOutputTrackerMasterEndpoint&lt;/span&gt; &lt;span class="nx"&gt;stopped&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;29&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;MemoryStore&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;MemoryStore&lt;/span&gt; &lt;span class="nx"&gt;cleared&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;29&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;BlockManager&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;BlockManager&lt;/span&gt; &lt;span class="nx"&gt;stopped&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;29&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;BlockManagerMaster&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;BlockManagerMaster&lt;/span&gt; &lt;span class="nx"&gt;stopped&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;29&lt;/span&gt; &lt;span class="nx"&gt;WARN&lt;/span&gt; &lt;span class="nx"&gt;MetricsSystem&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Stopping&lt;/span&gt; &lt;span class="nx"&gt;a&lt;/span&gt; &lt;span class="nx"&gt;MetricsSystem&lt;/span&gt; &lt;span class="nx"&gt;that&lt;/span&gt; &lt;span class="nx"&gt;is&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="nx"&gt;running&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;29&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;OutputCommitCoordinator&lt;/span&gt;&lt;span class="nv"&gt;$OutputCommitCoordinatorEndpoint&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;OutputCommitCoordinator&lt;/span&gt; &lt;span class="nx"&gt;stopped&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;29&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;SparkContext&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Successfully&lt;/span&gt; &lt;span class="nx"&gt;stopped&lt;/span&gt; &lt;span class="nx"&gt;SparkContext&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;29&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;RemoteActorRefProvider&lt;/span&gt;&lt;span class="nv"&gt;$RemotingTerminator&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Shutting&lt;/span&gt; &lt;span class="nx"&gt;down&lt;/span&gt; &lt;span class="nx"&gt;remote&lt;/span&gt; &lt;span class="nx"&gt;daemon.&lt;/span&gt;
&lt;span class="nx"&gt;Traceback&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;most&lt;/span&gt; &lt;span class="nx"&gt;recent&lt;/span&gt; &lt;span class="nx"&gt;call&lt;/span&gt; &lt;span class="nx"&gt;last&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="nb"&gt;File&lt;/span&gt; &lt;span class="s2"&gt;"c:/spark/spark-1.6.2-bin-hadoop2.6/examples/src/main/python/wordcount.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;line&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;module&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;29&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;RemoteActorRefProvider&lt;/span&gt;&lt;span class="nv"&gt;$RemotingTerminator&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Remote&lt;/span&gt; &lt;span class="nx"&gt;daemon&lt;/span&gt; &lt;span class="nx"&gt;shut&lt;/span&gt; &lt;span class="nx"&gt;down&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="nx"&gt;proceeding&lt;/span&gt; &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nx"&gt;flushing&lt;/span&gt; &lt;span class="nx"&gt;remote&lt;/span&gt; &lt;span class="nx"&gt;transports.&lt;/span&gt;
    &lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="o"&gt; =&lt;/span&gt; &lt;span class="nx"&gt;SparkContext&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;appName&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"PythonWordCount"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="nb"&gt;File&lt;/span&gt; &lt;span class="s2"&gt;"c:\spark\spark-1.6.2-bin-hadoop2.6\python\lib\pyspark.zip\pyspark\context.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;line&lt;/span&gt; &lt;span class="mi"&gt;115&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="nx"&gt;__init__&lt;/span&gt;
  &lt;span class="nb"&gt;File&lt;/span&gt; &lt;span class="s2"&gt;"c:\spark\spark-1.6.2-bin-hadoop2.6\python\lib\pyspark.zip\pyspark\context.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;line&lt;/span&gt; &lt;span class="mi"&gt;172&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="nx"&gt;_do_init&lt;/span&gt;
  &lt;span class="nb"&gt;File&lt;/span&gt; &lt;span class="s2"&gt;"c:\spark\spark-1.6.2-bin-hadoop2.6\python\lib\pyspark.zip\pyspark\context.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;line&lt;/span&gt; &lt;span class="mi"&gt;235&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="nx"&gt;_initialize_context&lt;/span&gt;
  &lt;span class="nb"&gt;File&lt;/span&gt; &lt;span class="s2"&gt;"c:\spark\spark-1.6.2-bin-hadoop2.6\python\lib\py4j-0.9-src.zip\py4j\java_gateway.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;line&lt;/span&gt; &lt;span class="mi"&gt;1064&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="nx"&gt;__call__&lt;/span&gt;
  &lt;span class="nb"&gt;File&lt;/span&gt; &lt;span class="s2"&gt;"c:\spark\spark-1.6.2-bin-hadoop2.6\python\lib\py4j-0.9-src.zip\py4j\protocol.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;line&lt;/span&gt; &lt;span class="mi"&gt;308&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="nx"&gt;get_return_value&lt;/span&gt;
&lt;span class="nx"&gt;py4j.protocol.Py4JJavaError&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;An&lt;/span&gt; &lt;span class="nx"&gt;error&lt;/span&gt; &lt;span class="nx"&gt;occurred&lt;/span&gt; &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="nx"&gt;calling&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="bp"&gt;.&lt;/span&gt;&lt;span class="nx nx-Member"&gt;org.apache.spark.api.java.JavaSparkContext.&lt;/span&gt;
&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;java.lang.NullPointerException&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;java.lang.ProcessBuilder.start&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Unknown&lt;/span&gt; &lt;span class="nx"&gt;Source&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.hadoop.util.Shell.runCommand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Shell.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;482&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.hadoop.util.Shell.run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Shell.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;455&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.hadoop.util.Shell&lt;/span&gt;&lt;span class="nv"&gt;$ShellCommandExecutor.execute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Shell.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;715&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.hadoop.fs.FileUtil.chmod&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;FileUtil.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;873&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.hadoop.fs.FileUtil.chmod&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;FileUtil.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;853&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.spark.util.Utils&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="bp"&gt;.&lt;/span&gt;&lt;span class="nx nx-Member"&gt;fetchFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Utils.scala&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;407&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.spark.SparkContext.addFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;SparkContext.scala&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1386&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.spark.SparkContext.addFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;SparkContext.scala&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1340&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.spark.SparkContext&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;$anonfun&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="bp"&gt;.&lt;/span&gt;&lt;span class="nx nx-Member"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;SparkContext.scala&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;491&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.spark.SparkContext&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;$anonfun&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="bp"&gt;.&lt;/span&gt;&lt;span class="nx nx-Member"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;SparkContext.scala&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;491&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;scala.collection.immutable.List.foreach&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;List&lt;/span&gt;&lt;span class="bp"&gt;.&lt;/span&gt;&lt;span class="nx nx-Member"&gt;scala&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;318&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.spark.SparkContext.&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;init&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;SparkContext.scala&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;491&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.spark.api.java.JavaSparkContext.&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;init&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;JavaSparkContext.scala&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;59&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;sun.reflect.NativeConstructorAccessorImpl.newInstance0&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Native&lt;/span&gt; &lt;span class="nx"&gt;Method&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;sun.reflect.NativeConstructorAccessorImpl.newInstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Unknown&lt;/span&gt; &lt;span class="nx"&gt;Source&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;sun.reflect.DelegatingConstructorAccessorImpl.newInstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Unknown&lt;/span&gt; &lt;span class="nx"&gt;Source&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;java.lang.reflect.Constructor.newInstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Unknown&lt;/span&gt; &lt;span class="nx"&gt;Source&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;py4j.reflection.MethodInvoker.invoke&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;MethodInvoker.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;234&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;py4j.reflection.ReflectionEngine.invoke&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ReflectionEngine.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;381&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;py4j.Gateway.invoke&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Gateway.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;214&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;py4j.commands.ConstructorCommand.invokeConstructor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ConstructorCommand.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;79&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;py4j.commands.ConstructorCommand.execute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ConstructorCommand.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;68&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;py4j.GatewayConnection.run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;GatewayConnection.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;209&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;java.lang.Thread.run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Unknown&lt;/span&gt; &lt;span class="nx"&gt;Source&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;29&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;ShutdownHookManager&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Shutdown&lt;/span&gt; &lt;span class="nx"&gt;hook&lt;/span&gt; &lt;span class="nx"&gt;called&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;29&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;RemoteActorRefProvider&lt;/span&gt;&lt;span class="nv"&gt;$RemotingTerminator&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Remoting&lt;/span&gt; &lt;span class="nx"&gt;shut&lt;/span&gt; &lt;span class="nx"&gt;down.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="installing-winutils"&gt;Installing winutils&lt;/h3&gt;
&lt;p&gt;Let’s download the &lt;code&gt;winutils.exe&lt;/code&gt; and configure our Spark installation to find &lt;code&gt;winutils.exe&lt;/code&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Create a &lt;code&gt;hadoop\bin&lt;/code&gt; folder inside the SPARK_HOME folder.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Download the &lt;a href="http://github.com/steveloughran/winutils"&gt;winutils.exe&lt;/a&gt; for the version of hadoop against which your Spark installation was built for. In my case the hadoop version was 2.6.0. So I &lt;a href="http://github.com/steveloughran/winutils/raw/master/hadoop-2.6.0/bin/winutils.exe"&gt;downloaded&lt;/a&gt; the winutils.exe for hadoop 2.6.0 and copied it to the &lt;code&gt;hadoop\bin&lt;/code&gt; folder in the SPARK_HOME folder.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create a system environment variable in Windows called &lt;code&gt;SPARK_HOME&lt;/code&gt; that points to the SPARK_HOME folder path. Search the internet in case you need a refresher on how to create environment variables in your version of Windows such as articles like &lt;a href="http://www.computerhope.com/issues/ch000549.htm"&gt;these&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create another system environment variable in Windows called &lt;code&gt;HADOOP_HOME&lt;/code&gt; that points to the hadoop folder inside the SPARK_HOME folder. &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Since the &lt;code&gt;hadoop&lt;/code&gt; folder is inside the SPARK_HOME folder, it is better to create &lt;code&gt;HADOOP_HOME&lt;/code&gt; environment variable using a value of &lt;code&gt;%SPARK_HOME%\hadoop&lt;/code&gt;. That way you don’t have to change &lt;code&gt;HADOOP_HOME&lt;/code&gt; if &lt;code&gt;SPARK_HOME&lt;/code&gt; is updated.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you now run the &lt;code&gt;bin\pyspark&lt;/code&gt; script from a Windows Command Prompt, the error messages related to &lt;code&gt;winutils.exe&lt;/code&gt; should be gone. For example, I got the following messages after running the &lt;code&gt;bin\pyspark&lt;/code&gt; utility after configuring &lt;code&gt;winutils&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;Python&lt;/span&gt; &lt;span class="nt"&gt;2&lt;/span&gt;&lt;span class="nc"&gt;.7.10&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;default&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;May&lt;/span&gt; &lt;span class="nt"&gt;23&lt;/span&gt; &lt;span class="nt"&gt;2015&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;09&lt;/span&gt;&lt;span class="nd"&gt;:44:00&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;MSC&lt;/span&gt; &lt;span class="nx"&gt;v.1500&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt; &lt;span class="nx"&gt;bit&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;AMD64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; &lt;span class="nt"&gt;on&lt;/span&gt; &lt;span class="nt"&gt;win32&lt;/span&gt;
&lt;span class="nt"&gt;Type&lt;/span&gt; &lt;span class="s2"&gt;"help"&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"copyright"&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"credits"&lt;/span&gt; &lt;span class="nt"&gt;or&lt;/span&gt; &lt;span class="s2"&gt;"license"&lt;/span&gt; &lt;span class="nt"&gt;for&lt;/span&gt; &lt;span class="nt"&gt;more&lt;/span&gt; &lt;span class="nt"&gt;information&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;span class="nt"&gt;Using&lt;/span&gt; &lt;span class="nt"&gt;Spark&lt;/span&gt;&lt;span class="s1"&gt;'s default log4j profile: org/apache/spark/log4j-defaults.properties&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 16:37:51 INFO SparkContext: Running Spark version 1.6.2&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 16:37:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 16:37:52 INFO SecurityManager: Changing view acls to: deel4986&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 16:37:52 INFO SecurityManager: Changing modify acls to: deel4986&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 16:37:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(deel4986); users with modify permissions: Set(deel4986)&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 16:37:52 INFO Utils: Successfully started service '&lt;/span&gt;&lt;span class="nt"&gt;sparkDriver&lt;/span&gt;&lt;span class="err"&gt;'&lt;/span&gt; &lt;span class="nt"&gt;on&lt;/span&gt; &lt;span class="nt"&gt;port&lt;/span&gt; &lt;span class="nt"&gt;62029&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;span class="nt"&gt;16&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;07&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;09&lt;/span&gt; &lt;span class="nt"&gt;16&lt;/span&gt;&lt;span class="nd"&gt;:37:52&lt;/span&gt; &lt;span class="nt"&gt;INFO&lt;/span&gt; &lt;span class="nt"&gt;Slf4jLogger&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;Slf4jLogger&lt;/span&gt; &lt;span class="nt"&gt;started&lt;/span&gt;
&lt;span class="nt"&gt;16&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;07&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;09&lt;/span&gt; &lt;span class="nt"&gt;16&lt;/span&gt;&lt;span class="nd"&gt;:37:52&lt;/span&gt; &lt;span class="nt"&gt;INFO&lt;/span&gt; &lt;span class="nt"&gt;Remoting&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;Starting&lt;/span&gt; &lt;span class="nt"&gt;remoting&lt;/span&gt;
&lt;span class="nt"&gt;16&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;07&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;09&lt;/span&gt; &lt;span class="nt"&gt;16&lt;/span&gt;&lt;span class="nd"&gt;:37:52&lt;/span&gt; &lt;span class="nt"&gt;INFO&lt;/span&gt; &lt;span class="nt"&gt;Remoting&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;Remoting&lt;/span&gt; &lt;span class="nt"&gt;started&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="nt"&gt;listening&lt;/span&gt; &lt;span class="nt"&gt;on&lt;/span&gt; &lt;span class="nt"&gt;addresses&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;akka.tcp&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="c1"&gt;//sparkDriverActorSystem@localhost:62042]&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;37&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;52&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;Utils&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Successfully&lt;/span&gt; &lt;span class="nx"&gt;started&lt;/span&gt; &lt;span class="nx"&gt;service&lt;/span&gt; &lt;span class="s1"&gt;'sparkDriverActorSystem'&lt;/span&gt; &lt;span class="k"&gt;on&lt;/span&gt; &lt;span class="nx"&gt;port&lt;/span&gt; &lt;span class="mi"&gt;62042&lt;/span&gt;&lt;span class="nx"&gt;.&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;37&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;52&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;SparkEnv&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Registering&lt;/span&gt; &lt;span class="nx"&gt;MapOutputTracker&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;37&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;52&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;SparkEnv&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Registering&lt;/span&gt; &lt;span class="nx"&gt;BlockManagerMaster&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;37&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;52&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;MemoryStore&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;MemoryStore&lt;/span&gt; &lt;span class="nx"&gt;started&lt;/span&gt; &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nx"&gt;capacity&lt;/span&gt; &lt;span class="mf"&gt;511.1&lt;/span&gt; &lt;span class="nx"&gt;MB&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;37&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;53&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;SparkEnv&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Registering&lt;/span&gt; &lt;span class="nx"&gt;OutputCommitCoordinator&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;37&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;53&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;Utils&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Successfully&lt;/span&gt; &lt;span class="nx"&gt;started&lt;/span&gt; &lt;span class="nx"&gt;service&lt;/span&gt; &lt;span class="s1"&gt;'SparkUI'&lt;/span&gt; &lt;span class="k"&gt;on&lt;/span&gt; &lt;span class="nx"&gt;port&lt;/span&gt; &lt;span class="mi"&gt;4040&lt;/span&gt;&lt;span class="nx"&gt;.&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;37&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;53&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;SparkUI&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Started&lt;/span&gt; &lt;span class="nx"&gt;SparkUI&lt;/span&gt; &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;http&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="c1"&gt;//localhost:4040&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;37&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;53&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;Executor&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Starting&lt;/span&gt; &lt;span class="nx"&gt;executor&lt;/span&gt; &lt;span class="nx"&gt;ID&lt;/span&gt; &lt;span class="nx"&gt;driver&lt;/span&gt; &lt;span class="k"&gt;on&lt;/span&gt; &lt;span class="nx"&gt;host&lt;/span&gt; &lt;span class="nx"&gt;localhost&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;37&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;53&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;Utils&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Successfully&lt;/span&gt; &lt;span class="nx"&gt;started&lt;/span&gt; &lt;span class="nx"&gt;service&lt;/span&gt; &lt;span class="s1"&gt;'org.apache.spark.network.netty.NettyBlockTransferService'&lt;/span&gt; &lt;span class="k"&gt;on&lt;/span&gt; &lt;span class="nx"&gt;port&lt;/span&gt; &lt;span class="mi"&gt;62079&lt;/span&gt;&lt;span class="nx"&gt;.&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;37&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;53&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;NettyBlockTransferService&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Server&lt;/span&gt; &lt;span class="nx"&gt;created&lt;/span&gt; &lt;span class="k"&gt;on&lt;/span&gt; &lt;span class="mi"&gt;62079&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;37&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;53&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;BlockManagerMaster&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Trying&lt;/span&gt; &lt;span class="k"&gt;to&lt;/span&gt; &lt;span class="nb"&gt;register&lt;/span&gt; &lt;span class="nx"&gt;BlockManager&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;37&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;53&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;BlockManagerMasterEndpoint&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Registering&lt;/span&gt; &lt;span class="nx"&gt;block&lt;/span&gt; &lt;span class="nx"&gt;manager&lt;/span&gt; &lt;span class="nx"&gt;localhost&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;62079&lt;/span&gt; &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="mf"&gt;511.1&lt;/span&gt; &lt;span class="nx"&gt;MB&lt;/span&gt; &lt;span class="nx"&gt;RAM&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;BlockManagerId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;driver&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;localhost&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;62079&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;37&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;53&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;BlockManagerMaster&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Registered&lt;/span&gt; &lt;span class="nx"&gt;BlockManager&lt;/span&gt;
&lt;span class="nx"&gt;Welcome&lt;/span&gt; &lt;span class="k"&gt;to&lt;/span&gt;
      &lt;span class="nx"&gt;____&lt;/span&gt;              &lt;span class="nx"&gt;__&lt;/span&gt;
     &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="nx"&gt;__&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;__&lt;/span&gt;  &lt;span class="nx"&gt;___&lt;/span&gt; &lt;span class="nx"&gt;_____&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;__&lt;/span&gt;
    &lt;span class="nx"&gt;_&lt;/span&gt;&lt;span class="o"&gt;\&lt;/span&gt; &lt;span class="o"&gt;\/&lt;/span&gt; &lt;span class="nx"&gt;_&lt;/span&gt; &lt;span class="o"&gt;\/&lt;/span&gt; &lt;span class="nx"&gt;_&lt;/span&gt; &lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="nx"&gt;__&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;  &lt;span class="s1"&gt;'_/&lt;/span&gt;
&lt;span class="s1"&gt;   /__ / .__/\_,_/_/ /_/\_\   version 1.6.2&lt;/span&gt;
&lt;span class="s1"&gt;      /_/&lt;/span&gt;

&lt;span class="s1"&gt;Using Python version 2.7.10 (default, May 23 2015 09:44:00)&lt;/span&gt;
&lt;span class="s1"&gt;SparkContext available as sc, HiveContext available as sqlContext.&lt;/span&gt;
&lt;span class="s1"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The &lt;code&gt;bin\spark-submit&lt;/code&gt; utility can also be successfully used to run &lt;code&gt;wordcount.py&lt;/code&gt; script.&lt;/p&gt;
&lt;h3 id="configuring-the-log-level-for-spark"&gt;Configuring the log level for Spark&lt;/h3&gt;
&lt;p&gt;There are still a lot of extra &lt;span class="caps"&gt;INFO&lt;/span&gt; messages in the console everytime you start or exit from a PySpark shell or run the &lt;code&gt;spark-submit&lt;/code&gt; utility. So let’s make one more change to our Spark installation so that only warning and error messages are written to the console. In order to do this&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Copy the &lt;code&gt;log4j.properties.template&lt;/code&gt; file in the &lt;code&gt;SPARK_HOME\conf&lt;/code&gt; folder as &lt;code&gt;log4j.properties&lt;/code&gt; file in the &lt;code&gt;SPARK_HOME\conf&lt;/code&gt; folder.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set the &lt;code&gt;log4j.rootCategory&lt;/code&gt; property value to &lt;code&gt;WARN, console&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Save the &lt;code&gt;log4j.properties&lt;/code&gt; file.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now any informative messages will not be logged to the console. For example, I got the following messages after running the &lt;code&gt;bin\pyspark&lt;/code&gt; utility once I configured the log level to &lt;span class="caps"&gt;WARN&lt;/span&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Python 2.7.10 (default, May 23 2015, 09:44:00) [MSC v.1500 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license" for more information.
16/07/09 16:45:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 1.6.2
      /_/

Using Python version 2.7.10 (default, May 23 2015 09:44:00)
SparkContext available as sc, HiveContext available as sqlContext.
&amp;gt;&amp;gt;&amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="summary"&gt;Summary&lt;/h2&gt;
&lt;p&gt;In order to work with PySpark, start a Windows Command Prompt and change into your SPARK_HOME directory.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;To start a PySpark shell, run the &lt;code&gt;bin\pyspark&lt;/code&gt; utility. Once your are in the PySpark shell use the &lt;code&gt;sc&lt;/code&gt; and &lt;code&gt;sqlContext&lt;/code&gt; names and type &lt;code&gt;exit()&lt;/code&gt; to return back to the Command Prompt.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To run a standalone Python script, run the &lt;code&gt;bin\spark-submit&lt;/code&gt; utility and specify the path of your Python script as well as any arguments your Python script needs in the Command Prompt. For example, to run the &lt;code&gt;wordcount.py&lt;/code&gt; script from &lt;code&gt;examples&lt;/code&gt; directory in your SPARK_HOME folder, you can run the following command&lt;/p&gt;
&lt;p&gt;&lt;code&gt;bin\spark-submit examples\src\main\python\wordcount.py README.md&lt;/code&gt; &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;I used the following references to gather information about this post.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://blogs.msdn.microsoft.com/arsen/2016/02/09/resolving-spark-1-6-0-java-lang-nullpointerexception-not-found-value-sqlcontext-error-when-running-spark-shell-on-windows-10-64-bit/"&gt;Setting up winutils.exe&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Downloading Spark and Getting Started (chapter 2) from O’Reilly’s &lt;a href="https://www.amazon.com/dp/1449358624"&gt;Learning Spark&lt;/a&gt; book.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary><category term="pyspark"></category><category term="spark"></category><category term="python"></category></entry></feed>