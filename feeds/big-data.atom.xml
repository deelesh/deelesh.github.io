<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>My Weblog</title><link href="http://deelesh.github.io/" rel="alternate"></link><link href="http://deelesh.github.io/feeds/big-data.atom.xml" rel="self"></link><id>http://deelesh.github.io/</id><updated>2016-07-09T22:45:00-07:00</updated><entry><title>Getting Started with PySpark on Windows</title><link href="http://deelesh.github.io/pyspark-windows.html" rel="alternate"></link><published>2016-07-09T22:45:00-07:00</published><author><name>Deelesh Mandloi</name></author><id>tag:deelesh.github.io,2016-07-09:pyspark-windows.html</id><summary type="html">&lt;p&gt;I decided to teach myself how to work with big data and came across &lt;a href="http://spark.apache.org"&gt;Apache Spark&lt;/a&gt;. While I had heard of &lt;a href="http://hadoop.apache.org"&gt;Apache Hadoop&lt;/a&gt;, to use Hadoop for working with big data, I had to write code in Java which I was not really looking forward to as I love to write code in Python. Spark supports a Python programming &lt;span class="caps"&gt;API&lt;/span&gt; called &lt;a href="http://spark.apache.org/docs/latest/api/python/pyspark.html"&gt;PySpark&lt;/a&gt; that is actively maintained and was enough to convince me to start learning PySpark for working with big data.&lt;/p&gt;
&lt;p&gt;In this post, I describe how I got started with PySpark on Windows. My laptop is running Windows 10. So the screenshots are specific to Windows 10. I am also assuming that you are comfortable working with the Command Prompt on Windows. You do not have to be an expert, but you need to know how to start a Command Prompt and run commands such as those that help you move around your computer’s file system. In case you need a refresher, a quick &lt;a href="http://www.cs.princeton.edu/courses/archive/spr05/cos126/cmd-prompt.html"&gt;introduction&lt;/a&gt; might be handy.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Often times, many open source projects do not have good Windows support. So I had to first figure out if Spark and PySpark would work well on Windows. The official Spark &lt;a href="http://spark.apache.org/docs/latest/#downloading"&gt;documentation&lt;/a&gt; does mention about supporting Windows.   &lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id="installing-prerequisites"&gt;Installing Prerequisites&lt;/h2&gt;
&lt;p&gt;PySpark requires Java version 7 or later and Python version 2.6 or later. Let’s first check if they are already installed or install them and make sure that PySpark can work with these two components.&lt;/p&gt;
&lt;h3 id="java"&gt;Java&lt;/h3&gt;
&lt;p&gt;Java is used by many other software. So it is quite possible that a required version (in our case version 7 or later) is already available on your computer. To check if Java is available and find it’s version, open a Command Prompt and type the following command.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    java -version
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If Java is installed and configured to work from a Command Prompt, running the above command should print the information about the Java version to the console. For example, I got the following output on my laptop.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;   java version "1.8.0_92"
   Java(TM) SE Runtime Environment (build 1.8.0_92-b14)
   Java HotSpot(TM) 64-Bit Server VM (build 25.92-b14, mixed mode)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Instead if you get a message like&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    'java' is not recognized as an internal or external command, operable program or batch file.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It means you need to install Java. To do so, &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Go to the Java &lt;a href="http://www.oracle.com/technetwork/java/javase/downloads/"&gt;download&lt;/a&gt; page. In case the download link has changed, search for &lt;code&gt;Java SE Runtime Environment&lt;/code&gt; on the internet and you should be able to find the download page.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Click the &lt;em&gt;Download&lt;/em&gt; button beneath &lt;em&gt;&lt;span class="caps"&gt;JRE&lt;/span&gt;&lt;/em&gt; &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Accept the license agreement and download the latest version of &lt;code&gt;Java SE Runtime Environment&lt;/code&gt; installer. I suggest getting the exe for Windows x64 (such as &lt;code&gt;jre-8u92-windows-x64.exe&lt;/code&gt;) unless you are using a 32 bit version of Windows in which case you need to get the &lt;em&gt;Windows x86 Offline&lt;/em&gt; version.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run the installer. &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;After the installation is complete, close the Command Prompt if it was already open, open it and check if you can successfully run &lt;code&gt;java -version&lt;/code&gt; command.&lt;/p&gt;
&lt;h3 id="python"&gt;Python&lt;/h3&gt;
&lt;p&gt;Python is used by many other software. So it is quite possible that a required version (in our case version 2.6 or later) is already available on your computer. To check if Python is available and find it’s version, open a Command Prompt and type the following command.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    python --version
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If Python is installed and configured to work from a Command Prompt, running the above command should print the information about the Python version to the console. For example, I got the following output on my laptop.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;   Python 2.7.10
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Instead if you get a message like&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    'python' is not recognized as an internal or external command, operable program or batch file.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It means you need to install Python. To do so,&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Go to the Python &lt;a href="https://www.python.org/downloads/windows/"&gt;download&lt;/a&gt; page.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Click the &lt;em&gt;Latest Python 2 Release&lt;/em&gt; link.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Download the &lt;code&gt;Windows x86-64 MSI installer&lt;/code&gt; file. If you are using a 32 bit version of Windows download the &lt;code&gt;Windows x86 MSI installer&lt;/code&gt; file.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When you run the installer, on the &lt;strong&gt;Customize Python&lt;/strong&gt; section, make sure that the option &lt;em&gt;Add python.exe to Path&lt;/em&gt; is selected. If this option is not selected, some of the PySpark utilities such as &lt;code&gt;pyspark&lt;/code&gt; and &lt;code&gt;spark-submit&lt;/code&gt; might not work.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Add python.exe to Path when installing Python on Windows" src="images/python-install.png"/&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;After the installation is complete, close the Command Prompt if it was already open, open it and check if you can successfully run &lt;code&gt;python --version&lt;/code&gt; command.&lt;/p&gt;
&lt;h2 id="installing-apache-spark"&gt;Installing Apache Spark&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Go to the Spark &lt;a href="http://spark.apache.org/downloads.html"&gt;download&lt;/a&gt; page.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For &lt;em&gt;Choose a Spark release&lt;/em&gt;, select the latest stable release of Spark.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For &lt;em&gt;Choose a package type&lt;/em&gt;, select a version that is pre-built for the latest version of Hadoop such as &lt;em&gt;Pre-built for Hadoop 2.6&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For &lt;em&gt;Choose a download type&lt;/em&gt;, select &lt;em&gt;Direct Download&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Click the link next to &lt;em&gt;Download Spark&lt;/em&gt; to download a zipped tarball file ending in .tgz extension such as &lt;code&gt;spark-1.6.2-bin-hadoop2.6.tgz&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In order to install Apache Spark, there is no need to run any installer. You can extract the files from the downloaded tarball in any folder of your choice using the &lt;a href="http://www.7-zip.org/"&gt;7Zip&lt;/a&gt; tool. &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Make sure that the folder path and the folder name containing Spark files do not contain any spaces.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In my case, I created a folder called &lt;code&gt;spark&lt;/code&gt; on my C drive and extracted the zipped tarball in a folder called &lt;code&gt;spark-1.6.2-bin-hadoop2.6&lt;/code&gt;. So all Spark files are in a folder called &lt;code&gt;C:\spark\spark-1.6.2-bin-hadoop2.6&lt;/code&gt;. From now on, I will refer to this folder as &lt;code&gt;SPARK_HOME&lt;/code&gt; in this post.&lt;/p&gt;
&lt;p&gt;To test if your installation was successful, open a Command Prompt, change to SPARK_HOME directory and type &lt;code&gt;bin\pyspark&lt;/code&gt;. This should start the PySpark shell which can be used to interactively work with Spark. I got the following messages in the console after running &lt;code&gt;bin\pyspark&lt;/code&gt; command.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;Python&lt;/span&gt; &lt;span class="nt"&gt;2&lt;/span&gt;&lt;span class="nc"&gt;.7.10&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;default&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;May&lt;/span&gt; &lt;span class="nt"&gt;23&lt;/span&gt; &lt;span class="nt"&gt;2015&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;09&lt;/span&gt;&lt;span class="nd"&gt;:44:00&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;MSC&lt;/span&gt; &lt;span class="nx"&gt;v.1500&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt; &lt;span class="nx"&gt;bit&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;AMD64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; &lt;span class="nt"&gt;on&lt;/span&gt; &lt;span class="nt"&gt;win32&lt;/span&gt;
&lt;span class="nt"&gt;Type&lt;/span&gt; &lt;span class="s2"&gt;"help"&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"copyright"&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"credits"&lt;/span&gt; &lt;span class="nt"&gt;or&lt;/span&gt; &lt;span class="s2"&gt;"license"&lt;/span&gt; &lt;span class="nt"&gt;for&lt;/span&gt; &lt;span class="nt"&gt;more&lt;/span&gt; &lt;span class="nt"&gt;information&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;span class="nt"&gt;Using&lt;/span&gt; &lt;span class="nt"&gt;Spark&lt;/span&gt;&lt;span class="s1"&gt;'s default log4j profile: org/apache/spark/log4j-defaults.properties&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 15:44:10 INFO SparkContext: Running Spark version 1.6.2&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 15:44:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 15:44:10 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path&lt;/span&gt;
&lt;span class="s1"&gt;java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:355)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:370)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.util.Shell.&amp;lt;clinit&amp;gt;(Shell.java:363)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.util.StringUtils.&amp;lt;clinit&amp;gt;(StringUtils.java:79)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:104)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.security.Groups.&amp;lt;init&amp;gt;(Groups.java:86)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.security.Groups.&amp;lt;init&amp;gt;(Groups.java:66)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:280)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:271)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:248)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:763)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:748)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:621)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2198)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2198)&lt;/span&gt;
&lt;span class="s1"&gt;        at scala.Option.getOrElse(Option.scala:120)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2198)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.spark.SparkContext.&amp;lt;init&amp;gt;(SparkContext.scala:322)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.spark.api.java.JavaSparkContext.&amp;lt;init&amp;gt;(JavaSparkContext.scala:59)&lt;/span&gt;
&lt;span class="s1"&gt;        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)&lt;/span&gt;
&lt;span class="s1"&gt;        at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)&lt;/span&gt;
&lt;span class="s1"&gt;        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)&lt;/span&gt;
&lt;span class="s1"&gt;        at java.lang.reflect.Constructor.newInstance(Unknown Source)&lt;/span&gt;
&lt;span class="s1"&gt;        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)&lt;/span&gt;
&lt;span class="s1"&gt;        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)&lt;/span&gt;
&lt;span class="s1"&gt;        at py4j.Gateway.invoke(Gateway.java:214)&lt;/span&gt;
&lt;span class="s1"&gt;        at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)&lt;/span&gt;
&lt;span class="s1"&gt;        at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)&lt;/span&gt;
&lt;span class="s1"&gt;        at py4j.GatewayConnection.run(GatewayConnection.java:209)&lt;/span&gt;
&lt;span class="s1"&gt;        at java.lang.Thread.run(Unknown Source)&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 15:44:10 INFO SecurityManager: Changing view acls to: deel4986&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 15:44:10 INFO SecurityManager: Changing modify acls to: deel4986&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 15:44:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(deel4986); users with modify permissions: Set(deel4986)&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 15:44:11 INFO Utils: Successfully started service '&lt;/span&gt;&lt;span class="nt"&gt;sparkDriver&lt;/span&gt;&lt;span class="err"&gt;'&lt;/span&gt; &lt;span class="nt"&gt;on&lt;/span&gt; &lt;span class="nt"&gt;port&lt;/span&gt; &lt;span class="nt"&gt;53607&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;span class="nt"&gt;16&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;07&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;09&lt;/span&gt; &lt;span class="nt"&gt;15&lt;/span&gt;&lt;span class="nd"&gt;:44:11&lt;/span&gt; &lt;span class="nt"&gt;INFO&lt;/span&gt; &lt;span class="nt"&gt;Slf4jLogger&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;Slf4jLogger&lt;/span&gt; &lt;span class="nt"&gt;started&lt;/span&gt;
&lt;span class="nt"&gt;16&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;07&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;09&lt;/span&gt; &lt;span class="nt"&gt;15&lt;/span&gt;&lt;span class="nd"&gt;:44:11&lt;/span&gt; &lt;span class="nt"&gt;INFO&lt;/span&gt; &lt;span class="nt"&gt;Remoting&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;Starting&lt;/span&gt; &lt;span class="nt"&gt;remoting&lt;/span&gt;
&lt;span class="nt"&gt;16&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;07&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;09&lt;/span&gt; &lt;span class="nt"&gt;15&lt;/span&gt;&lt;span class="nd"&gt;:44:11&lt;/span&gt; &lt;span class="nt"&gt;INFO&lt;/span&gt; &lt;span class="nt"&gt;Remoting&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;Remoting&lt;/span&gt; &lt;span class="nt"&gt;started&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="nt"&gt;listening&lt;/span&gt; &lt;span class="nt"&gt;on&lt;/span&gt; &lt;span class="nt"&gt;addresses&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;akka.tcp&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="c1"&gt;//sparkDriverActorSystem@localhost:53620]&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;Utils&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Successfully&lt;/span&gt; &lt;span class="nx"&gt;started&lt;/span&gt; &lt;span class="nx"&gt;service&lt;/span&gt; &lt;span class="s1"&gt;'sparkDriverActorSystem'&lt;/span&gt; &lt;span class="k"&gt;on&lt;/span&gt; &lt;span class="nx"&gt;port&lt;/span&gt; &lt;span class="mi"&gt;53620&lt;/span&gt;&lt;span class="nx"&gt;.&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;SparkEnv&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Registering&lt;/span&gt; &lt;span class="nx"&gt;MapOutputTracker&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;SparkEnv&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Registering&lt;/span&gt; &lt;span class="nx"&gt;BlockManagerMaster&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;DiskBlockManager&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Created&lt;/span&gt; &lt;span class="kd"&gt;local&lt;/span&gt; &lt;span class="nx"&gt;directory&lt;/span&gt; &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;C&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="nx"&gt;Users&lt;/span&gt;&lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="nx"&gt;deel4986&lt;/span&gt;&lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="nx"&gt;AppData&lt;/span&gt;&lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="kd"&gt;Local&lt;/span&gt;&lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="nx"&gt;Temp&lt;/span&gt;&lt;span class="o"&gt;\&lt;/span&gt;&lt;span class="nx"&gt;blockmgr&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="nx"&gt;c931369&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;8987&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="nx"&gt;e52&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;9562&lt;/span&gt;&lt;span class="na"&gt;-f3e561aad111&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;MemoryStore&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;MemoryStore&lt;/span&gt; &lt;span class="nx"&gt;started&lt;/span&gt; &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nx"&gt;capacity&lt;/span&gt; &lt;span class="mf"&gt;511.1&lt;/span&gt; &lt;span class="nx"&gt;MB&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;SparkEnv&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Registering&lt;/span&gt; &lt;span class="nx"&gt;OutputCommitCoordinator&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;Utils&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Successfully&lt;/span&gt; &lt;span class="nx"&gt;started&lt;/span&gt; &lt;span class="nx"&gt;service&lt;/span&gt; &lt;span class="s1"&gt;'SparkUI'&lt;/span&gt; &lt;span class="k"&gt;on&lt;/span&gt; &lt;span class="nx"&gt;port&lt;/span&gt; &lt;span class="mi"&gt;4040&lt;/span&gt;&lt;span class="nx"&gt;.&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;SparkUI&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Started&lt;/span&gt; &lt;span class="nx"&gt;SparkUI&lt;/span&gt; &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;http&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="c1"&gt;//localhost:4040&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;Executor&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Starting&lt;/span&gt; &lt;span class="nx"&gt;executor&lt;/span&gt; &lt;span class="nx"&gt;ID&lt;/span&gt; &lt;span class="nx"&gt;driver&lt;/span&gt; &lt;span class="k"&gt;on&lt;/span&gt; &lt;span class="nx"&gt;host&lt;/span&gt; &lt;span class="nx"&gt;localhost&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;Utils&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Successfully&lt;/span&gt; &lt;span class="nx"&gt;started&lt;/span&gt; &lt;span class="nx"&gt;service&lt;/span&gt; &lt;span class="s1"&gt;'org.apache.spark.network.netty.NettyBlockTransferService'&lt;/span&gt; &lt;span class="k"&gt;on&lt;/span&gt; &lt;span class="nx"&gt;port&lt;/span&gt; &lt;span class="mi"&gt;53657&lt;/span&gt;&lt;span class="nx"&gt;.&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;NettyBlockTransferService&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Server&lt;/span&gt; &lt;span class="nx"&gt;created&lt;/span&gt; &lt;span class="k"&gt;on&lt;/span&gt; &lt;span class="mi"&gt;53657&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;BlockManagerMaster&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Trying&lt;/span&gt; &lt;span class="k"&gt;to&lt;/span&gt; &lt;span class="nb"&gt;register&lt;/span&gt; &lt;span class="nx"&gt;BlockManager&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;BlockManagerMasterEndpoint&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Registering&lt;/span&gt; &lt;span class="nx"&gt;block&lt;/span&gt; &lt;span class="nx"&gt;manager&lt;/span&gt; &lt;span class="nx"&gt;localhost&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;53657&lt;/span&gt; &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="mf"&gt;511.1&lt;/span&gt; &lt;span class="nx"&gt;MB&lt;/span&gt; &lt;span class="nx"&gt;RAM&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;BlockManagerId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;driver&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;localhost&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;53657&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;BlockManagerMaster&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Registered&lt;/span&gt; &lt;span class="nx"&gt;BlockManager&lt;/span&gt;
&lt;span class="nx"&gt;Welcome&lt;/span&gt; &lt;span class="k"&gt;to&lt;/span&gt;
      &lt;span class="nx"&gt;____&lt;/span&gt;              &lt;span class="nx"&gt;__&lt;/span&gt;
     &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="nx"&gt;__&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;__&lt;/span&gt;  &lt;span class="nx"&gt;___&lt;/span&gt; &lt;span class="nx"&gt;_____&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;__&lt;/span&gt;
    &lt;span class="nx"&gt;_&lt;/span&gt;&lt;span class="o"&gt;\&lt;/span&gt; &lt;span class="o"&gt;\/&lt;/span&gt; &lt;span class="nx"&gt;_&lt;/span&gt; &lt;span class="o"&gt;\/&lt;/span&gt; &lt;span class="nx"&gt;_&lt;/span&gt; &lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="nx"&gt;__&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;  &lt;span class="s1"&gt;'_/&lt;/span&gt;
&lt;span class="s1"&gt;   /__ / .__/\_,_/_/ /_/\_\   version 1.6.2&lt;/span&gt;
&lt;span class="s1"&gt;      /_/&lt;/span&gt;

&lt;span class="s1"&gt;Using Python version 2.7.10 (default, May 23 2015 09:44:00)&lt;/span&gt;
&lt;span class="s1"&gt;SparkContext available as sc, HiveContext available as sqlContext.&lt;/span&gt;
&lt;span class="s1"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The last message provides a hint on how to work with Spark in the PySpark shell using the &lt;code&gt;sc&lt;/code&gt; or &lt;code&gt;sqlContext&lt;/code&gt; names. For example, typing &lt;code&gt;sc.version&lt;/code&gt; in the shell should print the version of Spark. You can exit from the PySpark shell in the same way you exit from any Python shell by typing &lt;code&gt;exit()&lt;/code&gt;. &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The PySpark shell outputs a few messages on exit. So you need to hit enter to get back to the Command Prompt.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="configuring-the-spark-installation"&gt;Configuring the Spark Installation&lt;/h2&gt;
&lt;p&gt;Starting the PySpark shell produces a lot of messages of type &lt;span class="caps"&gt;INFO&lt;/span&gt;, &lt;span class="caps"&gt;ERROR&lt;/span&gt; and &lt;span class="caps"&gt;WARN&lt;/span&gt;. In this section we will see how to remove these messages. &lt;/p&gt;
&lt;p&gt;By default, the Spark installation on Windows does not include the &lt;code&gt;winutils.exe&lt;/code&gt; utility that is used by Spark. If you do not tell your Spark installation where to look for &lt;code&gt;winutils.exe&lt;/code&gt;, you will see error messages when running the PySpark shell such as&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This error message does not prevent the PySpark shell from starting. However if you try to run a standalone Python script using the &lt;code&gt;bin\spark-submit&lt;/code&gt; utility, you will get an error. For example, try running the &lt;code&gt;wordcount.py&lt;/code&gt; script from the &lt;code&gt;examples&lt;/code&gt; folder in the Command Prompt when you are in the SPARK_HOME directory.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    bin\spark-submit examples\src\main\python\wordcount.py README.md
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;which produces the following error that also points to missing &lt;code&gt;winutils.exe&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;Using&lt;/span&gt; &lt;span class="nt"&gt;Spark&lt;/span&gt;&lt;span class="s1"&gt;'s default log4j profile: org/apache/spark/log4j-defaults.properties&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 16:23:27 INFO SparkContext: Running Spark version 1.6.2&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 16:23:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 16:23:27 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path&lt;/span&gt;
&lt;span class="s1"&gt;java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:355)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:370)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.util.Shell.&amp;lt;clinit&amp;gt;(Shell.java:363)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.util.StringUtils.&amp;lt;clinit&amp;gt;(StringUtils.java:79)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:104)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.security.Groups.&amp;lt;init&amp;gt;(Groups.java:86)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.security.Groups.&amp;lt;init&amp;gt;(Groups.java:66)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:280)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:271)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:248)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:763)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:748)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:621)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2198)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2198)&lt;/span&gt;
&lt;span class="s1"&gt;        at scala.Option.getOrElse(Option.scala:120)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2198)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.spark.SparkContext.&amp;lt;init&amp;gt;(SparkContext.scala:322)&lt;/span&gt;
&lt;span class="s1"&gt;        at org.apache.spark.api.java.JavaSparkContext.&amp;lt;init&amp;gt;(JavaSparkContext.scala:59)&lt;/span&gt;
&lt;span class="s1"&gt;        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)&lt;/span&gt;
&lt;span class="s1"&gt;        at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)&lt;/span&gt;
&lt;span class="s1"&gt;        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)&lt;/span&gt;
&lt;span class="s1"&gt;        at java.lang.reflect.Constructor.newInstance(Unknown Source)&lt;/span&gt;
&lt;span class="s1"&gt;        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)&lt;/span&gt;
&lt;span class="s1"&gt;        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)&lt;/span&gt;
&lt;span class="s1"&gt;        at py4j.Gateway.invoke(Gateway.java:214)&lt;/span&gt;
&lt;span class="s1"&gt;        at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)&lt;/span&gt;
&lt;span class="s1"&gt;        at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)&lt;/span&gt;
&lt;span class="s1"&gt;        at py4j.GatewayConnection.run(GatewayConnection.java:209)&lt;/span&gt;
&lt;span class="s1"&gt;        at java.lang.Thread.run(Unknown Source)&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 16:23:27 INFO SecurityManager: Changing view acls to: deel4986&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 16:23:27 INFO SecurityManager: Changing modify acls to: deel4986&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 16:23:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(deel4986); users with modify permissions: Set(deel4986)&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 16:23:28 INFO Utils: Successfully started service '&lt;/span&gt;&lt;span class="nt"&gt;sparkDriver&lt;/span&gt;&lt;span class="err"&gt;'&lt;/span&gt; &lt;span class="nt"&gt;on&lt;/span&gt; &lt;span class="nt"&gt;port&lt;/span&gt; &lt;span class="nt"&gt;59506&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;span class="nt"&gt;16&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;07&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;09&lt;/span&gt; &lt;span class="nt"&gt;16&lt;/span&gt;&lt;span class="nd"&gt;:23:28&lt;/span&gt; &lt;span class="nt"&gt;INFO&lt;/span&gt; &lt;span class="nt"&gt;Slf4jLogger&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;Slf4jLogger&lt;/span&gt; &lt;span class="nt"&gt;started&lt;/span&gt;
&lt;span class="nt"&gt;16&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;07&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;09&lt;/span&gt; &lt;span class="nt"&gt;16&lt;/span&gt;&lt;span class="nd"&gt;:23:28&lt;/span&gt; &lt;span class="nt"&gt;INFO&lt;/span&gt; &lt;span class="nt"&gt;Remoting&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;Starting&lt;/span&gt; &lt;span class="nt"&gt;remoting&lt;/span&gt;
&lt;span class="nt"&gt;16&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;07&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;09&lt;/span&gt; &lt;span class="nt"&gt;16&lt;/span&gt;&lt;span class="nd"&gt;:23:28&lt;/span&gt; &lt;span class="nt"&gt;INFO&lt;/span&gt; &lt;span class="nt"&gt;Remoting&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;Remoting&lt;/span&gt; &lt;span class="nt"&gt;started&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="nt"&gt;listening&lt;/span&gt; &lt;span class="nt"&gt;on&lt;/span&gt; &lt;span class="nt"&gt;addresses&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;akka.tcp&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="c1"&gt;//sparkDriverActorSystem@localhost:59519]&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;Utils&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Successfully&lt;/span&gt; &lt;span class="nx"&gt;started&lt;/span&gt; &lt;span class="nx"&gt;service&lt;/span&gt; &lt;span class="s1"&gt;'sparkDriverActorSystem'&lt;/span&gt; &lt;span class="k"&gt;on&lt;/span&gt; &lt;span class="nx"&gt;port&lt;/span&gt; &lt;span class="mi"&gt;59519&lt;/span&gt;&lt;span class="nx"&gt;.&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;SparkEnv&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Registering&lt;/span&gt; &lt;span class="nx"&gt;MapOutputTracker&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;SparkEnv&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Registering&lt;/span&gt; &lt;span class="nx"&gt;BlockManagerMaster&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;MemoryStore&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;MemoryStore&lt;/span&gt; &lt;span class="nx"&gt;started&lt;/span&gt; &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nx"&gt;capacity&lt;/span&gt; &lt;span class="mf"&gt;511.1&lt;/span&gt; &lt;span class="nx"&gt;MB&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;SparkEnv&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Registering&lt;/span&gt; &lt;span class="nx"&gt;OutputCommitCoordinator&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;Utils&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Successfully&lt;/span&gt; &lt;span class="nx"&gt;started&lt;/span&gt; &lt;span class="nx"&gt;service&lt;/span&gt; &lt;span class="s1"&gt;'SparkUI'&lt;/span&gt; &lt;span class="k"&gt;on&lt;/span&gt; &lt;span class="nx"&gt;port&lt;/span&gt; &lt;span class="mi"&gt;4040&lt;/span&gt;&lt;span class="nx"&gt;.&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;SparkUI&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Started&lt;/span&gt; &lt;span class="nx"&gt;SparkUI&lt;/span&gt; &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;http&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="c1"&gt;//localhost:4040&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt; &lt;span class="nx"&gt;ERROR&lt;/span&gt; &lt;span class="nx"&gt;SparkContext&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Error&lt;/span&gt; &lt;span class="nx"&gt;initializing&lt;/span&gt; &lt;span class="nx"&gt;SparkContext.&lt;/span&gt;
&lt;span class="nx"&gt;java.lang.NullPointerException&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;java.lang.ProcessBuilder.start&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Unknown&lt;/span&gt; &lt;span class="nx"&gt;Source&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.hadoop.util.Shell.runCommand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Shell.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;482&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.hadoop.util.Shell.run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Shell.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;455&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.hadoop.util.Shell&lt;/span&gt;&lt;span class="nv"&gt;$ShellCommandExecutor.execute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Shell.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;715&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.hadoop.fs.FileUtil.chmod&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;FileUtil.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;873&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.hadoop.fs.FileUtil.chmod&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;FileUtil.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;853&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.spark.util.Utils&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="bp"&gt;.&lt;/span&gt;&lt;span class="nx nx-Member"&gt;fetchFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Utils.scala&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;407&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.spark.SparkContext.addFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;SparkContext.scala&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1386&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.spark.SparkContext.addFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;SparkContext.scala&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1340&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.spark.SparkContext&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;$anonfun&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="bp"&gt;.&lt;/span&gt;&lt;span class="nx nx-Member"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;SparkContext.scala&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;491&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.spark.SparkContext&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;$anonfun&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="bp"&gt;.&lt;/span&gt;&lt;span class="nx nx-Member"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;SparkContext.scala&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;491&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;scala.collection.immutable.List.foreach&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;List&lt;/span&gt;&lt;span class="bp"&gt;.&lt;/span&gt;&lt;span class="nx nx-Member"&gt;scala&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;318&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.spark.SparkContext.&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;init&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;SparkContext.scala&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;491&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.spark.api.java.JavaSparkContext.&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;init&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;JavaSparkContext.scala&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;59&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;sun.reflect.NativeConstructorAccessorImpl.newInstance0&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Native&lt;/span&gt; &lt;span class="nx"&gt;Method&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;sun.reflect.NativeConstructorAccessorImpl.newInstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Unknown&lt;/span&gt; &lt;span class="nx"&gt;Source&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;sun.reflect.DelegatingConstructorAccessorImpl.newInstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Unknown&lt;/span&gt; &lt;span class="nx"&gt;Source&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;java.lang.reflect.Constructor.newInstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Unknown&lt;/span&gt; &lt;span class="nx"&gt;Source&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;py4j.reflection.MethodInvoker.invoke&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;MethodInvoker.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;234&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;py4j.reflection.ReflectionEngine.invoke&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ReflectionEngine.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;381&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;py4j.Gateway.invoke&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Gateway.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;214&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;py4j.commands.ConstructorCommand.invokeConstructor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ConstructorCommand.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;79&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;py4j.commands.ConstructorCommand.execute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ConstructorCommand.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;68&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;py4j.GatewayConnection.run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;GatewayConnection.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;209&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;java.lang.Thread.run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Unknown&lt;/span&gt; &lt;span class="nx"&gt;Source&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;29&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;SparkUI&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Stopped&lt;/span&gt; &lt;span class="nx"&gt;Spark&lt;/span&gt; &lt;span class="nx"&gt;web&lt;/span&gt; &lt;span class="nx"&gt;UI&lt;/span&gt; &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;http&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="c1"&gt;//localhost:4040&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;29&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;MapOutputTrackerMasterEndpoint&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;MapOutputTrackerMasterEndpoint&lt;/span&gt; &lt;span class="nx"&gt;stopped&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;29&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;MemoryStore&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;MemoryStore&lt;/span&gt; &lt;span class="nx"&gt;cleared&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;29&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;BlockManager&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;BlockManager&lt;/span&gt; &lt;span class="nx"&gt;stopped&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;29&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;BlockManagerMaster&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;BlockManagerMaster&lt;/span&gt; &lt;span class="nx"&gt;stopped&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;29&lt;/span&gt; &lt;span class="nx"&gt;WARN&lt;/span&gt; &lt;span class="nx"&gt;MetricsSystem&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Stopping&lt;/span&gt; &lt;span class="nx"&gt;a&lt;/span&gt; &lt;span class="nx"&gt;MetricsSystem&lt;/span&gt; &lt;span class="nx"&gt;that&lt;/span&gt; &lt;span class="nx"&gt;is&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="nx"&gt;running&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;29&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;OutputCommitCoordinator&lt;/span&gt;&lt;span class="nv"&gt;$OutputCommitCoordinatorEndpoint&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;OutputCommitCoordinator&lt;/span&gt; &lt;span class="nx"&gt;stopped&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;29&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;SparkContext&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Successfully&lt;/span&gt; &lt;span class="nx"&gt;stopped&lt;/span&gt; &lt;span class="nx"&gt;SparkContext&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;29&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;RemoteActorRefProvider&lt;/span&gt;&lt;span class="nv"&gt;$RemotingTerminator&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Shutting&lt;/span&gt; &lt;span class="nx"&gt;down&lt;/span&gt; &lt;span class="nx"&gt;remote&lt;/span&gt; &lt;span class="nx"&gt;daemon.&lt;/span&gt;
&lt;span class="nx"&gt;Traceback&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;most&lt;/span&gt; &lt;span class="nx"&gt;recent&lt;/span&gt; &lt;span class="nx"&gt;call&lt;/span&gt; &lt;span class="nx"&gt;last&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="nb"&gt;File&lt;/span&gt; &lt;span class="s2"&gt;"c:/spark/spark-1.6.2-bin-hadoop2.6/examples/src/main/python/wordcount.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;line&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;module&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;29&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;RemoteActorRefProvider&lt;/span&gt;&lt;span class="nv"&gt;$RemotingTerminator&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Remote&lt;/span&gt; &lt;span class="nx"&gt;daemon&lt;/span&gt; &lt;span class="nx"&gt;shut&lt;/span&gt; &lt;span class="nx"&gt;down&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="nx"&gt;proceeding&lt;/span&gt; &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nx"&gt;flushing&lt;/span&gt; &lt;span class="nx"&gt;remote&lt;/span&gt; &lt;span class="nx"&gt;transports.&lt;/span&gt;
    &lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="o"&gt; =&lt;/span&gt; &lt;span class="nx"&gt;SparkContext&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;appName&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"PythonWordCount"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="nb"&gt;File&lt;/span&gt; &lt;span class="s2"&gt;"c:\spark\spark-1.6.2-bin-hadoop2.6\python\lib\pyspark.zip\pyspark\context.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;line&lt;/span&gt; &lt;span class="mi"&gt;115&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="nx"&gt;__init__&lt;/span&gt;
  &lt;span class="nb"&gt;File&lt;/span&gt; &lt;span class="s2"&gt;"c:\spark\spark-1.6.2-bin-hadoop2.6\python\lib\pyspark.zip\pyspark\context.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;line&lt;/span&gt; &lt;span class="mi"&gt;172&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="nx"&gt;_do_init&lt;/span&gt;
  &lt;span class="nb"&gt;File&lt;/span&gt; &lt;span class="s2"&gt;"c:\spark\spark-1.6.2-bin-hadoop2.6\python\lib\pyspark.zip\pyspark\context.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;line&lt;/span&gt; &lt;span class="mi"&gt;235&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="nx"&gt;_initialize_context&lt;/span&gt;
  &lt;span class="nb"&gt;File&lt;/span&gt; &lt;span class="s2"&gt;"c:\spark\spark-1.6.2-bin-hadoop2.6\python\lib\py4j-0.9-src.zip\py4j\java_gateway.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;line&lt;/span&gt; &lt;span class="mi"&gt;1064&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="nx"&gt;__call__&lt;/span&gt;
  &lt;span class="nb"&gt;File&lt;/span&gt; &lt;span class="s2"&gt;"c:\spark\spark-1.6.2-bin-hadoop2.6\python\lib\py4j-0.9-src.zip\py4j\protocol.py"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;line&lt;/span&gt; &lt;span class="mi"&gt;308&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="nx"&gt;get_return_value&lt;/span&gt;
&lt;span class="nx"&gt;py4j.protocol.Py4JJavaError&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;An&lt;/span&gt; &lt;span class="nx"&gt;error&lt;/span&gt; &lt;span class="nx"&gt;occurred&lt;/span&gt; &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="nx"&gt;calling&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="bp"&gt;.&lt;/span&gt;&lt;span class="nx nx-Member"&gt;org.apache.spark.api.java.JavaSparkContext.&lt;/span&gt;
&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;java.lang.NullPointerException&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;java.lang.ProcessBuilder.start&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Unknown&lt;/span&gt; &lt;span class="nx"&gt;Source&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.hadoop.util.Shell.runCommand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Shell.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;482&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.hadoop.util.Shell.run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Shell.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;455&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.hadoop.util.Shell&lt;/span&gt;&lt;span class="nv"&gt;$ShellCommandExecutor.execute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Shell.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;715&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.hadoop.fs.FileUtil.chmod&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;FileUtil.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;873&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.hadoop.fs.FileUtil.chmod&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;FileUtil.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;853&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.spark.util.Utils&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="bp"&gt;.&lt;/span&gt;&lt;span class="nx nx-Member"&gt;fetchFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Utils.scala&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;407&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.spark.SparkContext.addFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;SparkContext.scala&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1386&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.spark.SparkContext.addFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;SparkContext.scala&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1340&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.spark.SparkContext&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;$anonfun&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="bp"&gt;.&lt;/span&gt;&lt;span class="nx nx-Member"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;SparkContext.scala&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;491&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.spark.SparkContext&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;$anonfun&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="bp"&gt;.&lt;/span&gt;&lt;span class="nx nx-Member"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;SparkContext.scala&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;491&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;scala.collection.immutable.List.foreach&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;List&lt;/span&gt;&lt;span class="bp"&gt;.&lt;/span&gt;&lt;span class="nx nx-Member"&gt;scala&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;318&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.spark.SparkContext.&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;init&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;SparkContext.scala&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;491&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;org.apache.spark.api.java.JavaSparkContext.&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;init&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;JavaSparkContext.scala&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;59&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;sun.reflect.NativeConstructorAccessorImpl.newInstance0&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Native&lt;/span&gt; &lt;span class="nx"&gt;Method&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;sun.reflect.NativeConstructorAccessorImpl.newInstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Unknown&lt;/span&gt; &lt;span class="nx"&gt;Source&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;sun.reflect.DelegatingConstructorAccessorImpl.newInstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Unknown&lt;/span&gt; &lt;span class="nx"&gt;Source&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;java.lang.reflect.Constructor.newInstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Unknown&lt;/span&gt; &lt;span class="nx"&gt;Source&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;py4j.reflection.MethodInvoker.invoke&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;MethodInvoker.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;234&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;py4j.reflection.ReflectionEngine.invoke&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ReflectionEngine.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;381&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;py4j.Gateway.invoke&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Gateway.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;214&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;py4j.commands.ConstructorCommand.invokeConstructor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ConstructorCommand.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;79&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;py4j.commands.ConstructorCommand.execute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ConstructorCommand.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;68&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;py4j.GatewayConnection.run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;GatewayConnection.java&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;209&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;java.lang.Thread.run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Unknown&lt;/span&gt; &lt;span class="nx"&gt;Source&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;29&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;ShutdownHookManager&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Shutdown&lt;/span&gt; &lt;span class="nx"&gt;hook&lt;/span&gt; &lt;span class="nx"&gt;called&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;29&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;RemoteActorRefProvider&lt;/span&gt;&lt;span class="nv"&gt;$RemotingTerminator&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Remoting&lt;/span&gt; &lt;span class="nx"&gt;shut&lt;/span&gt; &lt;span class="nx"&gt;down.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="installing-winutils"&gt;Installing winutils&lt;/h3&gt;
&lt;p&gt;Let’s download the &lt;code&gt;winutils.exe&lt;/code&gt; and configure our Spark installation to find &lt;code&gt;winutils.exe&lt;/code&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Create a &lt;code&gt;hadoop\bin&lt;/code&gt; folder inside the SPARK_HOME folder.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Download the &lt;a href="http://github.com/steveloughran/winutils"&gt;winutils.exe&lt;/a&gt; for the version of hadoop against which your Spark installation was built for. In my case the hadoop version was 2.6.0. So I &lt;a href="http://github.com/steveloughran/winutils/raw/master/hadoop-2.6.0/bin/winutils.exe"&gt;downloaded&lt;/a&gt; the winutils.exe for hadoop 2.6.0 and copied it to the &lt;code&gt;hadoop\bin&lt;/code&gt; folder in the SPARK_HOME folder.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create a system environment variable in Windows called &lt;code&gt;SPARK_HOME&lt;/code&gt; that points to the SPARK_HOME folder path. Search the internet in case you need a refresher on how to create environment variables in your version of Windows such as articles like &lt;a href="http://www.computerhope.com/issues/ch000549.htm"&gt;these&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create another system environment variable in Windows called &lt;code&gt;HADOOP_HOME&lt;/code&gt; that points to the hadoop folder inside the SPARK_HOME folder. &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Since the &lt;code&gt;hadoop&lt;/code&gt; folder is inside the SPARK_HOME folder, it is better to create &lt;code&gt;HADOOP_HOME&lt;/code&gt; environment variable using a value of &lt;code&gt;%SPARK_HOME%\hadoop&lt;/code&gt;. That way you don’t have to change &lt;code&gt;HADOOP_HOME&lt;/code&gt; if &lt;code&gt;SPARK_HOME&lt;/code&gt; is updated.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you now run the &lt;code&gt;bin\pyspark&lt;/code&gt; script from a Windows Command Prompt, the error messages related to &lt;code&gt;winutils.exe&lt;/code&gt; should be gone. For example, I got the following messages after running the &lt;code&gt;bin\pyspark&lt;/code&gt; utility after configuring &lt;code&gt;winutils&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;Python&lt;/span&gt; &lt;span class="nt"&gt;2&lt;/span&gt;&lt;span class="nc"&gt;.7.10&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;default&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;May&lt;/span&gt; &lt;span class="nt"&gt;23&lt;/span&gt; &lt;span class="nt"&gt;2015&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;09&lt;/span&gt;&lt;span class="nd"&gt;:44:00&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;MSC&lt;/span&gt; &lt;span class="nx"&gt;v.1500&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt; &lt;span class="nx"&gt;bit&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;AMD64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; &lt;span class="nt"&gt;on&lt;/span&gt; &lt;span class="nt"&gt;win32&lt;/span&gt;
&lt;span class="nt"&gt;Type&lt;/span&gt; &lt;span class="s2"&gt;"help"&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"copyright"&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"credits"&lt;/span&gt; &lt;span class="nt"&gt;or&lt;/span&gt; &lt;span class="s2"&gt;"license"&lt;/span&gt; &lt;span class="nt"&gt;for&lt;/span&gt; &lt;span class="nt"&gt;more&lt;/span&gt; &lt;span class="nt"&gt;information&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;span class="nt"&gt;Using&lt;/span&gt; &lt;span class="nt"&gt;Spark&lt;/span&gt;&lt;span class="s1"&gt;'s default log4j profile: org/apache/spark/log4j-defaults.properties&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 16:37:51 INFO SparkContext: Running Spark version 1.6.2&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 16:37:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 16:37:52 INFO SecurityManager: Changing view acls to: deel4986&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 16:37:52 INFO SecurityManager: Changing modify acls to: deel4986&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 16:37:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(deel4986); users with modify permissions: Set(deel4986)&lt;/span&gt;
&lt;span class="s1"&gt;16/07/09 16:37:52 INFO Utils: Successfully started service '&lt;/span&gt;&lt;span class="nt"&gt;sparkDriver&lt;/span&gt;&lt;span class="err"&gt;'&lt;/span&gt; &lt;span class="nt"&gt;on&lt;/span&gt; &lt;span class="nt"&gt;port&lt;/span&gt; &lt;span class="nt"&gt;62029&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;span class="nt"&gt;16&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;07&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;09&lt;/span&gt; &lt;span class="nt"&gt;16&lt;/span&gt;&lt;span class="nd"&gt;:37:52&lt;/span&gt; &lt;span class="nt"&gt;INFO&lt;/span&gt; &lt;span class="nt"&gt;Slf4jLogger&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;Slf4jLogger&lt;/span&gt; &lt;span class="nt"&gt;started&lt;/span&gt;
&lt;span class="nt"&gt;16&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;07&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;09&lt;/span&gt; &lt;span class="nt"&gt;16&lt;/span&gt;&lt;span class="nd"&gt;:37:52&lt;/span&gt; &lt;span class="nt"&gt;INFO&lt;/span&gt; &lt;span class="nt"&gt;Remoting&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;Starting&lt;/span&gt; &lt;span class="nt"&gt;remoting&lt;/span&gt;
&lt;span class="nt"&gt;16&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;07&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nt"&gt;09&lt;/span&gt; &lt;span class="nt"&gt;16&lt;/span&gt;&lt;span class="nd"&gt;:37:52&lt;/span&gt; &lt;span class="nt"&gt;INFO&lt;/span&gt; &lt;span class="nt"&gt;Remoting&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;Remoting&lt;/span&gt; &lt;span class="nt"&gt;started&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="nt"&gt;listening&lt;/span&gt; &lt;span class="nt"&gt;on&lt;/span&gt; &lt;span class="nt"&gt;addresses&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;akka.tcp&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="c1"&gt;//sparkDriverActorSystem@localhost:62042]&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;37&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;52&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;Utils&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Successfully&lt;/span&gt; &lt;span class="nx"&gt;started&lt;/span&gt; &lt;span class="nx"&gt;service&lt;/span&gt; &lt;span class="s1"&gt;'sparkDriverActorSystem'&lt;/span&gt; &lt;span class="k"&gt;on&lt;/span&gt; &lt;span class="nx"&gt;port&lt;/span&gt; &lt;span class="mi"&gt;62042&lt;/span&gt;&lt;span class="nx"&gt;.&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;37&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;52&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;SparkEnv&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Registering&lt;/span&gt; &lt;span class="nx"&gt;MapOutputTracker&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;37&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;52&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;SparkEnv&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Registering&lt;/span&gt; &lt;span class="nx"&gt;BlockManagerMaster&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;37&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;52&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;MemoryStore&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;MemoryStore&lt;/span&gt; &lt;span class="nx"&gt;started&lt;/span&gt; &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nx"&gt;capacity&lt;/span&gt; &lt;span class="mf"&gt;511.1&lt;/span&gt; &lt;span class="nx"&gt;MB&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;37&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;53&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;SparkEnv&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Registering&lt;/span&gt; &lt;span class="nx"&gt;OutputCommitCoordinator&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;37&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;53&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;Utils&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Successfully&lt;/span&gt; &lt;span class="nx"&gt;started&lt;/span&gt; &lt;span class="nx"&gt;service&lt;/span&gt; &lt;span class="s1"&gt;'SparkUI'&lt;/span&gt; &lt;span class="k"&gt;on&lt;/span&gt; &lt;span class="nx"&gt;port&lt;/span&gt; &lt;span class="mi"&gt;4040&lt;/span&gt;&lt;span class="nx"&gt;.&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;37&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;53&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;SparkUI&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Started&lt;/span&gt; &lt;span class="nx"&gt;SparkUI&lt;/span&gt; &lt;span class="nx"&gt;at&lt;/span&gt; &lt;span class="nx"&gt;http&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="c1"&gt;//localhost:4040&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;37&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;53&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;Executor&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Starting&lt;/span&gt; &lt;span class="nx"&gt;executor&lt;/span&gt; &lt;span class="nx"&gt;ID&lt;/span&gt; &lt;span class="nx"&gt;driver&lt;/span&gt; &lt;span class="k"&gt;on&lt;/span&gt; &lt;span class="nx"&gt;host&lt;/span&gt; &lt;span class="nx"&gt;localhost&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;37&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;53&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;Utils&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Successfully&lt;/span&gt; &lt;span class="nx"&gt;started&lt;/span&gt; &lt;span class="nx"&gt;service&lt;/span&gt; &lt;span class="s1"&gt;'org.apache.spark.network.netty.NettyBlockTransferService'&lt;/span&gt; &lt;span class="k"&gt;on&lt;/span&gt; &lt;span class="nx"&gt;port&lt;/span&gt; &lt;span class="mi"&gt;62079&lt;/span&gt;&lt;span class="nx"&gt;.&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;37&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;53&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;NettyBlockTransferService&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Server&lt;/span&gt; &lt;span class="nx"&gt;created&lt;/span&gt; &lt;span class="k"&gt;on&lt;/span&gt; &lt;span class="mi"&gt;62079&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;37&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;53&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;BlockManagerMaster&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Trying&lt;/span&gt; &lt;span class="k"&gt;to&lt;/span&gt; &lt;span class="nb"&gt;register&lt;/span&gt; &lt;span class="nx"&gt;BlockManager&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;37&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;53&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;BlockManagerMasterEndpoint&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Registering&lt;/span&gt; &lt;span class="nx"&gt;block&lt;/span&gt; &lt;span class="nx"&gt;manager&lt;/span&gt; &lt;span class="nx"&gt;localhost&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;62079&lt;/span&gt; &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="mf"&gt;511.1&lt;/span&gt; &lt;span class="nx"&gt;MB&lt;/span&gt; &lt;span class="nx"&gt;RAM&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;BlockManagerId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;driver&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;localhost&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;62079&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;07&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;09&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;37&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;53&lt;/span&gt; &lt;span class="nx"&gt;INFO&lt;/span&gt; &lt;span class="nx"&gt;BlockManagerMaster&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Registered&lt;/span&gt; &lt;span class="nx"&gt;BlockManager&lt;/span&gt;
&lt;span class="nx"&gt;Welcome&lt;/span&gt; &lt;span class="k"&gt;to&lt;/span&gt;
      &lt;span class="nx"&gt;____&lt;/span&gt;              &lt;span class="nx"&gt;__&lt;/span&gt;
     &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="nx"&gt;__&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;__&lt;/span&gt;  &lt;span class="nx"&gt;___&lt;/span&gt; &lt;span class="nx"&gt;_____&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;__&lt;/span&gt;
    &lt;span class="nx"&gt;_&lt;/span&gt;&lt;span class="o"&gt;\&lt;/span&gt; &lt;span class="o"&gt;\/&lt;/span&gt; &lt;span class="nx"&gt;_&lt;/span&gt; &lt;span class="o"&gt;\/&lt;/span&gt; &lt;span class="nx"&gt;_&lt;/span&gt; &lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="nx"&gt;__&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;  &lt;span class="s1"&gt;'_/&lt;/span&gt;
&lt;span class="s1"&gt;   /__ / .__/\_,_/_/ /_/\_\   version 1.6.2&lt;/span&gt;
&lt;span class="s1"&gt;      /_/&lt;/span&gt;

&lt;span class="s1"&gt;Using Python version 2.7.10 (default, May 23 2015 09:44:00)&lt;/span&gt;
&lt;span class="s1"&gt;SparkContext available as sc, HiveContext available as sqlContext.&lt;/span&gt;
&lt;span class="s1"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The &lt;code&gt;bin\spark-submit&lt;/code&gt; utility can also be successfully used to run &lt;code&gt;wordcount.py&lt;/code&gt; script.&lt;/p&gt;
&lt;h3 id="configuring-the-log-level-for-spark"&gt;Configuring the log level for Spark&lt;/h3&gt;
&lt;p&gt;There are still a lot of extra &lt;span class="caps"&gt;INFO&lt;/span&gt; messages in the console everytime you start or exit from a PySpark shell or run the &lt;code&gt;spark-submit&lt;/code&gt; utility. So let’s make one more change to our Spark installation so that only warning and error messages are written to the console. In order to do this&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Copy the &lt;code&gt;log4j.properties.template&lt;/code&gt; file in the &lt;code&gt;SPARK_HOME\conf&lt;/code&gt; folder as &lt;code&gt;log4j.properties&lt;/code&gt; file in the &lt;code&gt;SPARK_HOME\conf&lt;/code&gt; folder.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set the &lt;code&gt;log4j.rootCategory&lt;/code&gt; property value to &lt;code&gt;WARN, console&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Save the &lt;code&gt;log4j.properties&lt;/code&gt; file.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now any informative messages will not be logged to the console. For example, I got the following messages after running the &lt;code&gt;bin\pyspark&lt;/code&gt; utility once I configured the log level to &lt;span class="caps"&gt;WARN&lt;/span&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Python 2.7.10 (default, May 23 2015, 09:44:00) [MSC v.1500 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license" for more information.
16/07/09 16:45:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 1.6.2
      /_/

Using Python version 2.7.10 (default, May 23 2015 09:44:00)
SparkContext available as sc, HiveContext available as sqlContext.
&amp;gt;&amp;gt;&amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="summary"&gt;Summary&lt;/h2&gt;
&lt;p&gt;In order to work with PySpark, start a Windows Command Prompt and change into your SPARK_HOME directory.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;To start a PySpark shell, run the &lt;code&gt;bin\pyspark&lt;/code&gt; utility. Once your are in the PySpark shell use the &lt;code&gt;sc&lt;/code&gt; and &lt;code&gt;sqlContext&lt;/code&gt; names and type &lt;code&gt;exit()&lt;/code&gt; to return back to the Command Prompt.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To run a standalone Python script, run the &lt;code&gt;bin\spark-submit&lt;/code&gt; utility and specify the path of your Python script as well as any arguments your Python script needs in the Command Prompt. For example, to run the &lt;code&gt;wordcount.py&lt;/code&gt; script from &lt;code&gt;examples&lt;/code&gt; directory in your SPARK_HOME folder, you can run the following command&lt;/p&gt;
&lt;p&gt;&lt;code&gt;bin\spark-submit examples\src\main\python\wordcount.py README.md&lt;/code&gt; &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;p&gt;I used the following references to gather information about this post.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://blogs.msdn.microsoft.com/arsen/2016/02/09/resolving-spark-1-6-0-java-lang-nullpointerexception-not-found-value-sqlcontext-error-when-running-spark-shell-on-windows-10-64-bit/"&gt;Setting up winutils.exe&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Downloading Spark and Getting Started (chapter 2) from O’Reilly’s &lt;a href="https://www.amazon.com/dp/1449358624"&gt;Learning Spark&lt;/a&gt; book.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary><category term="pyspark"></category><category term="spark"></category><category term="python"></category></entry></feed>