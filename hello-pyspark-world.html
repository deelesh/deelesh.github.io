<!DOCTYPE html>
<html lang="en-US">
    <head>
        <meta charset="utf-8"> 
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="Deelesh Mandloi" />
        <meta name="copyright" content="Deelesh Mandloi" />

        <meta name="twitter:creator" content="@deelesh">
        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="pyspark, spark, python, Big Data, " />

<meta property="og:title" content="Hello PySpark World "/>
<meta property="og:url" content="./hello-pyspark-world.html" />
<meta property="og:description" content="Writing Apache Spark applications using Python." />
<meta property="og:site_name" content="My Weblog" />
<meta property="og:article:author" content="Deelesh Mandloi" />
<meta property="og:article:published_time" content="2016-07-16T00:00:00-07:00" />
<meta property="" content="2016-07-16T00:00:00-07:00" />
<meta name="twitter:title" content="Hello PySpark World ">
<meta name="twitter:description" content="Writing Apache Spark applications using Python.">

        <title>Hello PySpark World  · My Weblog
</title>
        <link href="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/css/bootstrap-combined.min.css" rel="stylesheet">
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.1/css/font-awesome.css" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="./theme/css/pygments.css" media="screen">
        <link rel="stylesheet" type="text/css" href="./theme/tipuesearch/tipuesearch.css" media="screen">
        <link rel="stylesheet" type="text/css" href="./theme/css/elegant.css" media="screen">
        <link rel="stylesheet" type="text/css" href="./theme/css/custom.css" media="screen">
        <link href="http://deelesh.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="My Weblog - Full Atom Feed" />
    </head>
    <body>
        <div id="content-sans-footer">
        <div class="navbar navbar-static-top">
            <div class="navbar-inner">
                <div class="container-fluid">
                    <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </a>
                    <a class="brand" href="./"><span class=site-name>My Weblog</span></a>
                    <div class="nav-collapse collapse">
                        <ul class="nav pull-right top-menu">
                            <li ><a href=".">Home</a></li>
                            <li ><a href="./pages/projects.html">Projects</a></li>
                            <li ><a href="./categories.html">Categories</a></li>
                            <li ><a href="./tags.html">Tags</a></li>
                            <li ><a href="./archives.html">Archives</a></li>
                            <li><form class="navbar-search" action="./search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <div class="container-fluid">
            <div class="row-fluid">
                <div class="span1"></div>
                <div class="span10">
<article>
<div class="row-fluid">
    <header class="page-header span10 offset2">
    <h1><a href="./hello-pyspark-world.html"> Hello PySpark&nbsp;World  </a></h1>
    </header>
</div>

<div class="row-fluid">
    <div class="span2 table-of-content">
        <nav>
        <h4>Contents</h4>
        <div class="toc">
<ul>
<li><a href="#word-count-program">Word Count Program</a></li>
<li><a href="#running-word-count-program">Running Word Count Program</a></li>
<li><a href="#building-blocks-of-a-pyspark-program">Building Blocks of a PySpark Program</a></li>
<li><a href="#how-the-word-count-program-works">How the Word Count Program Works</a></li>
<li><a href="#summary">Summary</a></li>
</ul>
</div>
        </nav>
    </div>
    <div class="span8 article-content">

            
            <p>When learning <a href="http://spark.apache,org">Apache Spark</a>, the most common first example seems to be a program to count the number of words in a file. Let’s see how we can write such a program using the <a href="http://spark.apache.org/docs/latest/api/python/index.html">Python <span class="caps">API</span></a> for Spark (PySpark). This post assumes that you have already installed Spark. If you need a refresher on how to install Spark on Windows, checkout this <a href="pyspark-windows.html">post</a>.</p>

<h2 id="word-count-program">Word Count Program</h2>
<p>In this post we will learn how to write a program that counts the number of words in a file. To achieve this, the program needs to read the entire file, split each line on space and count the frequency of each unique word. Since I did not want to include a special file whose words our program can count, I am counting the words in the same file that contains the source code of our program. The entire program is listed below</p>
<div class="gist">
    <script src='https://gist.github.com/57778e3f3f4d4ff4a8e49985ca4dce06.js'></script>
    <noscript>
        <pre><code>'''Print the words and their frequencies in this file'''

import operator
import pyspark

def main():
    '''Program entry point'''

    #Intialize a spark context
    with pyspark.SparkContext("local", "PySparkWordCount") as sc:
        #Get a RDD containing lines from this script file  
        lines = sc.textFile(__file__)
        #Split each line into words and assign a frequency of 1 to each word
        words = lines.flatMap(lambda line: line.split(" ")).map(lambda word: (word, 1))
        #count the frequency for words
        counts = words.reduceByKey(operator.add)
        #Sort the counts in descending order based on the word frequency
        sorted_counts =  counts.sortBy(lambda x: x[1], False)
        #Get an iterator over the counts to print a word and its frequency
        for word,count in sorted_counts.toLocalIterator():
            print(u"{} --> {}".format(word, count))

if __name__ == "__main__":
    main()</code></pre>
    </noscript>
</div>
<h2 id="running-word-count-program">Running Word Count Program</h2>
<p>To run the Word Count program,</p>
<ol>
<li>
<p>Open a terminal window such as a Windows Command Prompt.</p>
</li>
<li>
<p>Change into your SPARK_HOME directory.</p>
</li>
<li>
<p>Run the <code>spark-submit</code> utility and pass the full path to your Word Count program file as an argument.</p>
</li>
</ol>
<p>For example, on my Windows laptop I used the following commands to run the Word Count program.</p>
<div class="highlight"><pre><span></span>    cd %SPARK_HOME%
    bin\spark-submit c:\code\pyspark-hello-world.py
</pre></div>
<h2 id="building-blocks-of-a-pyspark-program">Building Blocks of a PySpark Program</h2>
<p>In order to understand how the Word Count program works, we need to first understand the basic building blocks of any PySpark program. A PySpark program can be written using the following workflow </p>
<ul>
<li>
<p>Import the <a href="http://spark.apache.org/docs/latest/api/python/index.html">pyspark</a> Python module.</p>
</li>
<li>
<p>Create the <a href="http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext">SparkContext</a> by specifying the <span class="caps">URL</span> of the cluster on which to run your application and your application name.</p>
</li>
<li>
<p>Use one or more methods of the <code>SparkContext</code> to create a <a href="http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds">resilient distributed dataset</a> (<span class="caps">RDD</span>) from your big data.</p>
</li>
<li>
<p>Apply one or more transformations on your RDDs to process your big data.</p>
</li>
<li>
<p>Apply one or more actions on your RDDs to produce the outputs.    </p>
</li>
</ul>
<h2 id="how-the-word-count-program-works">How the Word Count Program Works</h2>
<p>Let’s see how we apply the PySpark workflow in our Word Count program. We first import the <code>pyspark</code> module along with the <a href="http://docs.python.org/2/library/operator.html">operator</a> module from the Python standard library as we need to later use the <code>add</code> function from the <code>operator</code> module.</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">operator</span>
<span class="kn">import</span> <span class="nn">pyspark</span>
</pre></div>
<p>Once the <code>pyspark</code> module is imported, we create a <code>SparkContext</code> instance passing in the special keyword string, <code>local</code>, and the name of our application, <code>PySparkWordCount</code>. The <code>local</code> keyword tells Spark to run this program locally in the same process that is used to run our program. Realistically you will specify the <span class="caps">URL</span> of the Spark cluster on which your application should run and not use the <code>local</code> keyword. The <code>SparkContext</code> is created using the <a href="http://docs.python.org/2/reference/compound_stmts.html#with">with statement</a> as the <code>SparkContext</code> needs to be closed when our program terminates.</p>
<div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pyspark</span><span class="o">.</span><span class="n">SparkContext</span><span class="p">(</span><span class="s2">"local"</span><span class="p">,</span> <span class="s2">"PySparkWordCount"</span><span class="p">)</span> <span class="k">as</span> <span class="n">sc</span><span class="p">:</span>
    <span class="c1">#Get a RDD containing lines from this script file</span>
</pre></div>
<p>Using the <a href="http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext.textFile">textFile</a> method on the <code>SparkContext</code> instance, we get a <span class="caps">RDD</span> containing all the lines from the program file. The path to the program file is obtained using <code>__file__</code> name.</p>
<div class="highlight"><pre><span></span><span class="n">lines</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="n">__file__</span><span class="p">)</span>
</pre></div>
<p>We then apply two transformations to the <code>lines</code> <span class="caps">RDD</span>. First we split each line using a space to get a <span class="caps">RDD</span> of all words in every line using the <a href="http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.flatMap">flatMap</a> transformation. Then we create a new <span class="caps">RDD</span> containing a list of  two value tuples where each tuple associates the number 1 with each word like <code>[(import 1), (operator, 1)]</code> using the <a href="http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map">map</a> transformation. </p>
<div class="highlight"><pre><span></span><span class="n">words</span> <span class="o">=</span> <span class="n">lines</span><span class="o">.</span><span class="n">flatMap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">line</span><span class="p">:</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">" "</span><span class="p">))</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">word</span><span class="p">:</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>
<p>Note the use of <a href="http://docs.python.org/2/tutorial/controlflow.html#lambda-expressions">lambda expression</a> in the <code>flatMap</code> and <code>map</code> transformations. Lambda expressions are used in Python to create anonymous functions at runtime without binding the functions to names. The above line could also be written as</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">split_line</span><span class="p">(</span><span class="n">line</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">" "</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">assign_one</span><span class="p">(</span><span class="n">word</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">words</span> <span class="o">=</span> <span class="n">lines</span><span class="o">.</span><span class="n">flatMap</span><span class="p">(</span><span class="n">split_line</span><span class="p">)</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">assign_one</span><span class="p">)</span>
</pre></div>
<p>If you are not used to lambda expressions, defining functions and then passing in function names to Spark transformations might make your code easier to read. But the Spark documentation seems to use lambda expressions in all of the Python examples. So it is better to get used to lambda expressions.</p>
<blockquote>
<p>Lambda expressions can have only one statement which returns the value. In case you need to have multiple statements in your functions, you need to use the pattern of defining explicit functions and passing in their names.</p>
</blockquote>
<p>We then apply the <a href="http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey">reduceByKey</a> transformation to the <code>words</code> <span class="caps">RDD</span> passing in the <code>add</code> function from the <code>operator</code> standard library module. This creates a new <span class="caps">RDD</span> that is like a dictionary with keys as unique words in the file and values as the frequency of the words.</p>
<div class="highlight"><pre><span></span><span class="n">counts</span> <span class="o">=</span> <span class="n">words</span><span class="o">.</span><span class="n">reduceByKey</span><span class="p">(</span><span class="n">operator</span><span class="o">.</span><span class="n">add</span><span class="p">)</span>
</pre></div>
<p>We then sort the <code>counts</code> <span class="caps">RDD</span> in the descending order based on the frequency of unique words such that words with highest frequency are listed first by applying the <a href="http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sortBy">sortyBy</a> transformation.</p>
<div class="highlight"><pre><span></span><span class="n">sorted_counts</span> <span class="o">=</span>  <span class="n">counts</span><span class="o">.</span><span class="n">sortBy</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">False</span><span class="p">)</span>
</pre></div>
<p>Finally we get an iterator over the <code>sorted_counts</code> <span class="caps">RDD</span> by applying the <a href="http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.toLocalIterator">toLocalIterator</a> action to print each unique word in the file and its frequency. </p>
<div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">word</span><span class="p">,</span><span class="n">count</span> <span class="ow">in</span> <span class="n">sorted_counts</span><span class="o">.</span><span class="n">toLocalIterator</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">u"{} --&gt; {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">count</span><span class="p">))</span>
</pre></div>
<blockquote>
<p>We are using the <code>toLocalIterator</code> action instead of the <a href="http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collect">collect</a> action as <code>collect</code> will return the entire list in memory which might cause an out of memory error if the input file is really big. By using the <code>toLocalIterator</code> action, our program will only hold a single word in memory at any time.</p>
</blockquote>
<h2 id="summary">Summary</h2>
<p>You can write PySpark programs by creating a <code>SparkContext</code>, loading your big data as an <span class="caps">RDD</span>, applying one or more transformations to the RDDs to perform your processing and applying one or more actions to the processed RDDs to get the results.</p>
            <section>
    <p id="post-share-links">
    Share on:
    <a href="http://twitter.com/home?status=Hello%20PySpark%C2%A0World%20http%3A//deelesh.github.io/hello-pyspark-world.html" target="_blank" title="Share on Twitter">Twitter</a>
    ❄
    <a href="http://www.facebook.com/sharer/sharer.php?u=http%3A//deelesh.github.io/hello-pyspark-world.html" target="_blank" title="Share on Facebook">Facebook</a>
    ❄
    <a href="https://plus.google.com/share?url=http%3A//deelesh.github.io/hello-pyspark-world.html" target="_blank" title="Share on Google Plus">Google+</a>
    ❄
    <a href="mailto:?subject=Hello%20PySpark%C2%A0World&amp;body=http%3A//deelesh.github.io/hello-pyspark-world.html" target="_blank" title="Share via Email">Email</a>
    </p>
</section>

            <section>
<p id="comment-message">Any suggestions or feedback? Leave your comments below. </p>
<div class="accordion" id="accordion2">
    <div class="accordion-group">
        <div class="accordion-heading">
            <a class="accordion-toggle disqus-comment-count" data-toggle="collapse" data-parent="#accordion2"
                    data-disqus-identifier="hello-pyspark-world-4cebc7d3a90149dd9a18c71266a29e17"
                href="./hello-pyspark-world.html#disqus_thread">
                Comments
            </a>
        </div>
        <div id="disqus_thread" class="accordion-body collapse">
            <div class="accordion-inner">
                <div class="comments">
                    <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'deelesh';
        var disqus_identifier = 'hello-pyspark-world-4cebc7d3a90149dd9a18c71266a29e17';
    var disqus_url = './hello-pyspark-world.html';

    (function() {
         var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
         dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
         (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
     })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

                </div>
            </div>
        </div>
    </div>
</div>
</section>

            <hr/>
            <aside>
            <nav>
            <ul class="articles-timeline">
                <li class="previous-article">« <a href="./pyspark-windows.html" title="Previous: Getting Started with PySpark on Windows">Getting Started with PySpark on Windows</a></li>
            </ul>
            </nav>
            </aside>
        </div>
        <section>
        <div class="span2" style="float:right;font-size:0.9em;">
            <h4>Published</h4>
            <time pubdate="pubdate" datetime="2016-07-16T00:00:00-07:00">Jul 16, 2016</time>

<h4>Last Updated</h4>
<time datetime="2016-07-16T00:00:00-07:00">Jul 16, 2016</time>

            <h4>Category</h4>
            <a class="category-link" href="./categories.html#big-data-ref">Big Data</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="./tags.html#pyspark-ref">pyspark
                    <span>2</span>
</a></li>
                <li><a href="./tags.html#python-ref">python
                    <span>2</span>
</a></li>
                <li><a href="./tags.html#spark-ref">spark
                    <span>2</span>
</a></li>
            </ul>
<h4>Contact</h4>
    <a href="http://github.com/deelesh" title="My github Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-github sidebar-social-links"></i></a>
    <a href="http://twitter.com/deelesh" title="My twitter Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-twitter sidebar-social-links"></i></a>
    <a href="mailto:deelesh@gmail.com" title="My Email Address" class="sidebar-social-links" target="_blank">
    <i class="fa fa-envelope sidebar-social-links"></i></a>
        </div>
        </section>
</div>
</article>
                </div>
                <div class="span1"></div>
            </div>
        </div>
        <div id="push"></div>
    </div>
<footer>
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-power">Powered by <a href="http://getpelican.com/" title="Pelican Home Page">Pelican</a>. Theme: <a href="http://oncrashreboot.com/pelican-elegant" title="Theme Elegant Home Page">Elegant</a> by <a href="http://oncrashreboot.com" title="Talha Mansoor Home Page">Talha Mansoor</a></li>
    </ul>
</div>
</footer>            <script src="http://code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>

            <script type="text/javascript">
var disqus_shortname = 'deelesh';
(function () {
    var s = document.createElement('script'); s.async = true;
    s.type = 'text/javascript';
    s.src = '//' + disqus_shortname + '.disqus.com/count.js';
    (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
}());
</script>
<script  language="javascript" type="text/javascript">
function uncollapse() {
    if (window.location.hash.match(/^#comment-\d+$/)) {
        $('#disqus_thread').collapse('show');
    }
}
</script>
<script type="text/javascript" language="JavaScript">
uncollapse();
window.onhashchange=function(){
    if (window.location.hash.match(/^#comment-\d+$/))
        window.location.reload(true);
}
</script>
<script>
$('#disqus_thread').on('shown', function () {
    var link = document.getElementsByClassName('accordion-toggle');
    var old_innerHTML = link[0].innerHTML;
    $(link[0]).fadeOut(500, function() {
        $(this).text('Click here to hide comments').fadeIn(500);
    });
    $('#disqus_thread').on('hidden', function () {
        $(link[0]).fadeOut(500, function() {
            $(this).text(old_innerHTML).fadeIn(500);
        });
    })
})
</script>


    </body>
    <!-- Theme: Elegant built for Pelican
    License : http://oncrashreboot.com/pelican-elegant -->
</html>